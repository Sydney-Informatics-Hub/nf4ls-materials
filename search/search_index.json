{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Nextflow for the Life Sciences 2025","text":"<p>This workshop will provide you with the foundational knowledge required to build Nextflow workflows. The content is broken up into 2 parts. In the first part we will cover the basic principles for developing Nextflow pipelines. In the second part we will step through building our own Nextflow workflow. See the event description and workshop schedule for learning outcomes and lesson plan, respectively.</p>"},{"location":"#trainers","title":"Trainers","text":"<ul> <li>Fred Jaya, Sydney Informatics Hub, University of Sydney</li> <li>Michael Geaghan, Sydney Informatics Hub, University of Sydney</li> </ul>"},{"location":"#facilitators","title":"Facilitators","text":"Site Facilitator(s) Brisbane (QCIF/QUT) Magdalena Antczak, Marie-Emilie Gauthier Perth (Pawsey) Sarah Beecroft, Pratihba Raghunandan Canberra (NCI) Kisaru Liyanage Adelaide (SAGC) John Salamon, Michael Roach Melbourne (Melbourne Bioinformatics) Emma Gail, Grace Hall Melbourne (Peter MacCallum Cancer Centre) Richard Lupat Sydney (Garvan Institute of Medical Research) Thanh Nguyen, Eric Urng, Matthew Hobbs"},{"location":"#prerequisites","title":"Prerequisites","text":"<p>This is an intermediate-advanced workshop for people developing reproducible bioinformatics workflows. It assumes some experience with the following:</p> <ul> <li>Experience working on the command line/Linux environment.</li> <li>Experience with basic scripting (e.g. Bash).</li> </ul> <p>In addition, experience with other reproducible workflow tools (e.g. CWL, WDL, or Snakemake) will be very useful, although not at all required for this workshop.</p>"},{"location":"#set-up-requirements","title":"Set up requirements","text":"<p>Please complete the Setup instruction before the workshop. If you have any trouble, please write in the discussion board on the Google document.</p>"},{"location":"#code-of-conduct","title":"Code of Conduct","text":"<p>In order to foster a positive and professional learning environment we encourage the following kinds of behaviours at all our events and on our platforms:</p> <ul> <li>Use welcoming and inclusive language</li> <li>Be respectful of different viewpoints and experiences</li> <li>Gracefully accept constructive criticism</li> <li>Focus on what is best for the community</li> <li>Show courtesy and respect towards other community members</li> <li>Our full code of conduct, with incident reporting guidelines, is available here.</li> </ul>"},{"location":"#workshop-schedule","title":"Workshop schedule","text":""},{"location":"#day-1","title":"Day 1","text":"Time (AEST) Activity Presenter(s) 13:00 Arrival Giorgia 13:15 Welcome and housekeeping Giorgia 13:25 Introductions and icebreakers Giorgia 13:40 Workshop overview and setup Fred 13:55 Introduction to Nextflow (Part 1.0) Fred 14:05 Workshop: Part 1.1 - 1.4 Fred 15:00 BREAK Giorgia 15:15 Workshop: Part 1.5 - 1.8 Fred 16:15 Q&amp;A and Day 1 recap Fred, Giorgia"},{"location":"#day-2","title":"Day 2","text":"Time (AEST) Activity Presenter(s) 13:00 Arrival Giorgia 13:10 Welcome and housekeeping Giorgia 13:15 Setup and introduction to Part 2 Michael 13:25 Workshop: Part 2.1 - 2.2 Michael 14:15 BREAK Giorgia 14:30 Workshop: Part 2.3 - 2.5 Michael 15:50 Day 2 summary Michael, Giorgia, Fred 16:00 Discussion and Q&amp;A Giorgia 16:15 Wrap up and feedback survey Giorgia"},{"location":"#course-survey","title":"Course survey","text":"<p>Please fill out our course survey at the end of the workshop. Help us help you! \ud83d\ude01</p>"},{"location":"#credits-and-acknowledgements","title":"Credits and acknowledgements","text":"<p>This workshop event and accompanying materials were developed by the Sydney Informatics Hub, University of Sydney. The workshop was enabled by Australian BioCommons' BioCLI Platforms Project (NCRIS via Bioplatforms Australia). Compute resources were provided by the National Computational Infrastructure (NCI).</p> <p>We gratefully acknowledge the contributions of Chris Hakkaart, Ziad Al-Bkhetan, Melissa Burke, Giorgia Mori, Uwe Winter, Georgie Samaha, Cali Willet, Mitchell O'Brien, Matthew Downton, and Wenjing Xue.</p> <p></p>"},{"location":"instructor/","title":"Instructor notes","text":""},{"location":"instructor/#overview","title":"Overview","text":"<p>Learning Nextflow for the first time can be daunting due to the number of features, settings, and concepts that new users need to learn. Many existing Nextflow documentation and training materials outline the things that you can do, but lacks clarity on the key ones required to get started with developing your own pipeline. </p> <p>This workshop helps life scientists overcome those barriers by: </p> <ul> <li>Focusing on the essential concepts required to build a functional pipeline </li> <li>Introducing these concepts sequentially, with each step building on earlier understanding </li> <li>Demonstrating these ideas first in a simple \u201ctoy\u201d workflow, then applying them to a realistic bioinformatics example </li> </ul> <p>The workshop is divided into two parts: </p> <ul> <li>Part 1: Foundations of Nextflow (processes, workflow scopes, channels, inputs/outputs, operators) </li> <li>Part 2: Applying these foundations to build a real-world RNA-seq workflow </li> </ul> <p>This workshop has been delivered as an online workshop, and as a hybrid format  </p> <p>The content is delivered as demonstrations (instructors guide participants through the rendered webpage) combined with hands-on exercises as either a code-along (follow the instructor), or independently (time is provided for learners to attempt exercises).</p>"},{"location":"instructor/#workshop-feedback-and-design","title":"Workshop feedback and design","text":"<p>This section reflects on the feedback and experience from delivering this content from past workshops. These notes are compiled directly from participant (learner) feedback, and discussion between trainers, facilitators, and coordinators.</p>"},{"location":"instructor/#to-keep","title":"To keep","text":"<p>Several elements of the current workshop have proven particularly effective and should be aimed to be kept in future delivery.</p> <p>Iterative, step-by-step code alone exercises</p> <p>The most consistent positive feedback relates to the incremental, guided nature of the materials. Each section builds directly on the previous one, helping learners form a model of how Nextflow pipelines are constructed that can be applied to their own development. As there are a lot of concepts that need to be selected from the documentation and introduced, this allows each of these to be explained in detail without overwhelming learners.  </p> <p>Opportunities for learners to code themselves</p> <p>Fold-out \u201csolution\u201d blocks work well: learners can attempt each exercise independently, then reveal the answer if needed. This supports mixed-experience groups while keeping that pace manageable. </p> <p>Practical guidelines and best practices</p> <p>Many aspects of effective Nextflow development (e.g., naming, structure, dataflow thinking) are not immediately obvious from the official documentation. The workshop introduces these patterns and provides guidelines on how to implement them. </p> <p>Pre-configured workshop environments</p> <p>The workshop needs the same folder structure and input files. Configuring learner compute environments on an Ubuntu VM (e.g. previously NCI Nirin, Nectar cloud) ensures all participants are using the same operating system, software version, and files. This has worked fantastically and allowed the workshop to focus on the delivery of teaching material. </p>"},{"location":"instructor/#to-improve","title":"To improve","text":"<p>While the workshop has been well-received, these are several areas that could be optionally improved to strengthen learning outcomes and meet sound curriculum design. </p> <p>Limited learning assessments and formative checks</p> <p>The workshop relies heavily on guided explanation and code-alongs, which works well given the conceptual load. However, this does not explicitly test understanding. Learning is determined when code is added correctly, and when pipelines run as intended - this does not guarantee learners understanding the introduced concept.  </p> <p>Suggested improvements: </p> <ul> <li>Develop small assessments to ensure learner understanding. For example, formative assessments throughout the workshop such as asking learners to reflect and note how they would apply concepts to their own pipelines; or inviting learners to reflect and note what they have learnt, want to know about further, and a question </li> </ul> <p>Polls are not effective</p> <p>Recurring feedback includes poll questions tend to be vague, and allowing enough time for learners to respond was insufficient. </p> <p>Suggested improvements: </p> <ul> <li>Consider how learners will access polls and ensure individual participant. i.e. can everyone submit a response?</li> <li>Replace polls with the formative checks mentioned above </li> </ul> <p>Part 1 is too slow</p> <p>The section on bash (Part 1.2) occupies too much time given that this is a prerequisite for learners. This was inherited from an early version of the workshop that was tightly linked to Seqera\u2019s Hello Nextflow training. </p> <p>Suggested improvements: </p> <ul> <li>Remove section 1.2, or move this to a brief overview at the start of the next section </li> <li>Use the gained time to focus on the Nextflow-specific concepts towards the end of Part 1 </li> </ul> <p>Other delivery formats</p> <p>This workshop has been delivered online, and as a hybrid distributed model. Instructions and material here have been adapted to suit these models and may not account for other formats.</p>"},{"location":"instructor/#preparing-to-deliver-the-workshop","title":"Preparing to deliver the workshop","text":"<p>There are several steps in order to prepare for workshop delivery.</p> <p>SIH and the Australian BioCommons are working towards providing access to a a pre-configured compute environments for self-directed learning, as well as setting up VMs.</p> <ol> <li>Familiarise yourself with the content by running through the workshop.</li> <li>Identify the compute resources and requirements for participants and training team.</li> </ol> <p>It is essential that all participants and trainers use the same version of software and have the same computational resources. We highly recommend you configure virtual machines for learners to access and use for the workshop exercises.  </p> <ol> <li> <p>Configure your own fork of the materials. See the how-to guide.</p> </li> <li> <p>Amend the rendered page with your workshop details (e.g. trainers and facilitators involved, delivery details, institution) </p> </li> </ol> <p>Optionally:</p> <ul> <li>Add, change, amend any content on your fork </li> <li> <p>Suggest changes to the nf4ls-materials content so future instructors can benefit from your improvements. See the contribution guidelines.</p> </li> <li> <p>On your selected infrastructure, follow the Setup instructions to install and  configure an environment with all software, containers, and files required for the workshop.</p> </li> </ul> <p>All other preparation, rehearsals to prepare for confident delivery </p>"},{"location":"instructor/#delivery-tips","title":"Delivery tips","text":"<p>This workshop has been delivered and adapted for online delivery, with lead trainers sharing their screen and communicating content via Zoom. These tips are adapted for this format, and may not fit other formats such as full in-person delivery with a single trainer. </p> <ul> <li>Have VSCode, and the rendered content on your screen as you deliver </li> <li>Trainers invite learners to copy and paste code blocks throughout the workshop for accuracy (i.e. avoid typos that result in pipeline errors). </li> <li>Trainers type out code additions and explain as they type. This allows trainers to provide more context and explain what is being added. Also helps to slow the pace of delivery to allow learners to catch up. </li> <li>Trainers verbally announce when they are moving between windows or tabs. For example \u201cI am now going to the VSCode terminal...\u201d. Participants will need to manage going between the workshop content in their browser, VSCode, and Zoom and this will help them follow you </li> </ul>"},{"location":"instructor/#lesson-guides","title":"Lesson guides","text":""},{"location":"instructor/#overall","title":"Overall","text":"<p>The goal of this workshop is to teach Nextflow best practices, and highlight the most important features to know and learn about amongst Nextflows large, and often daunting, repertoire. </p> <p>Whilst there is a lot of technical information that needs to be covered in detail, remember to tie it back to the biology - your audience are bioscientists that need to use Nextflow to process data and run analyses for their research or applied goals. </p> <p>For example, why would a biologist care about learning Nextflow over a set of functional bash scripts? Throughout the lessons, provide examples of where a particular feature would be useful in the context of a researcher running. These will be suggested in the lesson-specific subsections.</p>"},{"location":"instructor/#part-1-fundamental-nextflow-concepts","title":"Part 1 - Fundamental Nextflow Concepts","text":"Lesson Teaching Objective Learning Outcome Learning Experience Approx. Time 1.0 Introduction to Nextflow High-level overview of Nextflow and its uses Learners can identify the context of preliminary uses for bioinformatics pipelines Lecture 10 min 1.1-1.2 Introduciton to Part 1 and Hello World! Introduce the structure of the iterative pipeline for Part 1 and the bash commands used Learners can identify the high-level structure of piecing together bash commands in a pipeline Demonstration and code-along 15 min 1.3 Writing your first pipeline Introduce the minimum viable structure of a Nextflow pipeline Learners can identify and write a simple <code>process</code> and <code>workflow</code> Guided code-along building a minimal pipeline step-by-step 20 min min 1.4 Running pipelines Explain how Nextflow executes workflows and manages outputs Learners can run a pipeline, interpret logs, and understand <code>work</code>, caching, and <code>publishDir</code> Live execution, log walkthrough, discussion of outputs and caching 15 min 1.5 Inputs and channels Introduce channels as the core dataflow mechanism Learners understand why channels are required and how to pass simple inputs Conceptual explanation + small exercises creating and passing channels 20 min 1.6 Parameters Make pipelines flexible using parameters Learners can define and override parameters at runtime Code-along with parameterised examples and short experimentation 15 min 1.7 Adding processes Demonstrate chaining processes together Learners can connect processes using outputs as inputs Guided exercise building and wiring a second process 15 min 1.8 Dynamic naming Introduce dynamic output naming Learners can generate outputs that scale across inputs Short demonstration + modification of existing code 15 min"},{"location":"instructor/#13-writing-your-first-pipeline","title":"1.3 Writing your first pipeline","text":"<p>This is the first hands-on Nextflow exercise. In this lesson, you will guide learners through their initial Nextflow exercise by introducing them to the fundamental building blocks of a minimal Nextflow script. You'll cover the <code>process</code> and <code>workflow</code> scopes inside the <code>main.nf</code> file, focusing on key <code>process</code> components like <code>input</code>, <code>ouput</code>, and <code>script</code>. Your main aim is to ensure learners understand the minimum components needed for a simple, working, one-step pipeline.</p> <p>To repeat the content, this is important and the model will be used in later steps:</p> <p>The order of steps taken to build the process in the following exercises are intentional. We will be building processes by defining each process block starting with the script then the output. In later sections, we will add input and process directives to this order.</p> <p>This order is not prescriptive, however, the script logic often determines how the other process blocks should look like and this order can be helpful for breaking down building processes in a logical way. This approach will be continued in Part 2, when you build an RNAseq workflow!</p> <ul> <li>Highlight that this lesson is to introduce only the minimum requirements for a Nextflow script.</li> <li>Reinforce that there are many ways to achieve the same thing but it is important to be consistent (e.g. see individual concepts below).</li> <li>Ensure they grasp the core concept of having a <code>process</code> that does something simple, that is called in the <code>workflow</code>.</li> <li>You can use an analogy of <code>process</code> being similar to a function in R or Python, but you need to call the function to run it.</li> </ul> Syntax and stylingSingle vs. double quotes<code>script</code> block interpreter <p>There may be FAQs around Nextflow syntax and styling. For example, if indentation matters - it does in e.g. Python, but Nextflow is relaxed. There are many ways to style, indent, comment, and order code (e.g. process blocks: input, output, script), as well as place process scopes (modules vs. main.nf) and parameters (workflow{}, nextflow.config).</p> <ul> <li>Nextflow is flexible, you can do things differently and achieve the same outcome without error</li> <li>It is important to do things consistently for legibility</li> <li>This is especially important when you collaborate with others</li> <li>Recommend to adopt the approaches demonstrated in the workshop</li> </ul> <p>Double quotes must be used when a variable needs to be passed. Single quotes will interpret a string literally. For example:</p> <pre><code>- \"Hello ${name}\" -&gt; `Hello Fred`\n- 'Hello Fred' -&gt; `Hello Fred`\n- \"Hello Fred\" -&gt; `Hello Fred`\n- 'Hello ${name}' -&gt; `Hello ${name}`\"\n</code></pre> <p>Learners will gain practice with hands-on exercises requiring correct use of single/double quotes throughout the workshop.</p> <p>By default, the <code>script:</code> block in a process is executed with bash. It is possible to use a different interpreter via a shebang (e.g. <code>#</code>/usr/bin/env Rscript<code>but can introduce some added complexity. Whilst out of scope for this introductory workshop, suggest adding scripts in the  [</code>./bin` directory](https://training.nextflow.io/2.1/advanced/structure/#bin).</p>"},{"location":"instructor/#14-running-pipelines","title":"1.4 Running pipelines","text":"<p>In this lesson, you will guide learners through executing the basic pipeline developed in the previous section using <code>nextflow run</code>. The focus here is to highlight the **key outputs, and help learners understand these in detail. This includes the Nextflow log that is printed to the terminal, the importance of unique run names and hashes, and the files output by the pipeline such as the <code>work</code> directory. Once the <code>work</code> directory is explained, you will lead into how this is used as a cache for resuming workflows with the <code>-resume</code> flag. Finally, you will explain the <code>publishDir</code> directory to show how outputs can be organised more conveniently.</p> <ul> <li>After running a pipeline, teach learners how to read the terminal output and understand the key lines of the Nextflow log. Explain line-by-line and in detail.</li> <li>Emphasise the importance of the <code>work</code> directory for storing logs and how it plays a role in caching and resuming workflows</li> <li>Be direct with learners: Nextflow runs produces many files. Using <code>publishDir</code> is a best practice to keep the outputs you want organised, in a predictable place, especially as workflows scale up.</li> <li>This is a key feature that Nextflow has over, for example, bash scripts. You will need to manually write a checkpointing feature in bash, whereas it come \"out of the box\" in Nextflow.</li> </ul>"},{"location":"instructor/#15-inputs-and-channels","title":"1.5 Inputs and channels","text":"<p>This lesson introduces channels with exercises to create a channel, passing a string into a process, and updating the process to expect a string input.</p> <p>Emphasise that channels are one of the trickiest concepts to learn in Nextflow, but getting it right is crucial as it is what makes Nextflow efficient and reproducible. It can be tempting for new learners to manually process inputs and outputs within R or Python scripts (with exceptions: creating folders, moving files, error/file checking), but this must be done the Nextflow way, using channels.</p> <p>Reassure learners that this lesson will focus on a conceptual introduction to channels, and Part 2 will provide learners with opportunities to implement channels for different bioinfomatics use cases. There are different ways that channels should be used, and manipulated with groovy, but this is beyond the scope of this introductory workshop. Suggest useful resources such as Nextflow patterns which have examples of different use cases using channels. </p> <p>Key things to mention:</p> <ul> <li>Channels are the backbone of Nextflow workflows. It passes information (data, parameters, values) into and between processes. </li> <li>Channels are how Nextflow allows you to run jobs in parallel - when set up correctly Nextflow will automatically figure out which jobs are ready to be run.</li> <li>Clarify the difference between multiple inputs vs. multiple channels</li> <li>DO NOT use publishDir as an input to a process. publishDir is a way to organise important output files after jobs finish running. Use channels.</li> </ul>"},{"location":"instructor/#16-parameters","title":"1.6 Parameters","text":"<p>This lesson demonstrates how learners can leverage parameters to make your Nextflow pipelines flexible (not specififying fixed names, files, paths, or values) for many different files and options. The key idea to communicate is that we develop pipelines because, more often than not, need to run the same processing over many samples. If you develop the pipeline to work on one sample, it will need to be updated when the sample change. Parameters overcome this by allowing users to specify different values during runtime, or use a default if none is specified.</p> <p>Reinforce the use of parameters through the exercises. For the last exercise, allow 5 minutes for learners to experiment adding different values (strings) to the <code>--greeting</code> and <code>--outdir</code> parameters. Suggest that they can use the solution and inspect the output but to have a go themselves. Remind learners that multi-word strings must be enclosed in either single or double quotes.</p>"},{"location":"instructor/#17-adding-processes","title":"1.7 Adding processes","text":"<p>This lesson is the first demonstration of connecting processes, where the output of <code>SAYHELLO</code> is used as an input to a new <code>CONVERTOUPPER</code> process.</p> <p>Suggest spending minimal time demonstrating what <code>tr</code> does to allow learners more time to indpendently work through the exercise involving piecing together the <code>CONVERTTOUPPER</code> process. Encourage learners the use of hints.</p> <p>Recall that the order we suggest of adding a new process is reflected in the hints:</p> <ol> <li><code>script</code></li> <li><code>output</code></li> <li><code>input</code></li> <li>Updating <code>workflow{}</code></li> </ol> Accessing process outputs <p>There are different ways to access outputs:</p> <ul> <li>Adding <code>.out</code> to the end of a process name only works for single-element channels</li> <li>For single-element channels, using <code>.out[0]</code> is identical to <code>.out</code></li> <li><code>emit</code> can be used to name output</li> </ul>"},{"location":"instructor/#18-dynamic-naming","title":"1.8 Dynamic naming","text":"<p>Similar to lesson 1.6 Parameters, this lesson will guide learners to make pipelines more flexible and adaptable for use across samples by automatically (dynamically) naming output files based on the name of the input. This becomes important to correctly identify outputs when running across multiple samples or files.</p>"},{"location":"instructor/#part-2","title":"Part 2","text":"Lesson Teaching Objective Learning Outcome Learning Experience Approx. Time 2.0 Introduction Set context for Part 2 workflow Learners understand the scenario, data, and end goal Instructor walkthrough of data, tools, and workflow structure 10 min 2.1 First process &amp; containers Convert a bash script into a containerised process Learners can map bash scripts to Nextflow processes and use containers Code-along converting bash \u2192 process with container 20 min 2.2 Samplesheets, operators, Groovy Introduce structured inputs and data reshaping Learners understand why tuples and operators are required Concept-focused walkthrough with examples and debugging demos 30 min 2.3 Multiple process inputs Teach input structuring and process chaining Learners can decide when to group vs separate inputs Guided workflow wiring with discussion of design decisions 25 min 2.4 Combining channels &amp; outputs Demonstrate aggregation patterns Learners understand when aggregation is appropriate (e.g. MultiQC) Example-driven explanation using MultiQC 30 min 2.5 Upscaling &amp; introspection Show how pipelines scale without code changes Learners understand scaling via configuration and samplesheets Demonstration + discussion; pointer to HPC workshop 25 min Part 2 goals and scopeStructured and repeated development across lessons <p>Part 2 replicates a \"real-life\" example of how a bioscientist would encounter and code a Nextflow pipeline for the first time. The key goals of Part 2 include:</p> <ul> <li>Demonstrating how the foundational Nextflow concepts introduced in Part 1 is applied in a practical scenario</li> <li>Reiterate the key Nextflow concepts and components that are required for a simple bioinformatics workflow</li> </ul> <p>What Part 2 is not:</p> <ul> <li>A guide on bioinformatics or data analyses, reviewing each input or output files and the inner workings of tools in detail</li> </ul> <p>Each lesson (2.1 - 2.5) will focus on converting a bash script into a modular Nextflow process. This is to emphasise that there is some boilerplate that will not change across tools, scripts, and analyses in Nextflow pipelines. However, these must be tweaked to fit based on what will be input and output.</p> <p>For each lesson, you will guide learners to review:</p> <ol> <li>An existing bash script that conducts one bioinformatics data processing step</li> <li>Identify which components need to be implemented in a Nextflow process definition</li> <li>Build that Nextflow process, step-by-step, following a similar order</li> </ol>"},{"location":"instructor/#20-introduction","title":"2.0 Introduction","text":"<p>This section provides the context for Part 2, such as the scenario of converting individual bash scripts into a simple, end-to-end Nextflow pipeline. </p> <p>You will introduce:</p> <ul> <li>The files and data used for Part 2</li> <li>The bioinformatics tool</li> <li>Nextflow files that you will be changing</li> <li>The high-level structure of the workflow that learners will have built by the end of the workshop.</li> </ul>"},{"location":"instructor/#21-our-first-process-and-container","title":"2.1 Our first process and container","text":"Bash script to Nextflow processUsing containers in Nextflow <p>This is the first occurrence of reviewing a bash script and converting it to a Nextflow process. Recall that, when defining a process, there is some boilerplate code that is unlikely to change (<code>input:</code>, <code>output:</code>, <code>script:</code>), but the contents of each need to be tailored (based on the bash scripts). </p> <p>There are many ways to manage software (e.g. modules, conda) and we consider using containers in Nextflow is the best practice. Containers package a tool together with its software environment so that everyone runs the same tool, same version, same dependencies, regardless of where the workflow is executed. As a Nextflow pipeline has many steps requiring different tools, using containers simplifies this and makes your pipeline reproducible.</p> <p>There are many container platforms and repositories. For bioinformatics, particularly use in Nextflow, recommend learners to use Singularity (widely supported on HPC systems and suitable for shared environments) and BioContainers that are hosted on quay.io (plenty of pre-built containers for thousands of bioinformatics tools and supported by Nextflow, nf-core, Galaxy)</p> <p>Learners do not need to understand the inner working of containers. The key things to communicate is knowing:</p> <ul> <li>Why they are used</li> <li>Where bioinformatics containers can be searched and retrieved from</li> <li>How to implement a Nextflow process that uses a specific container</li> </ul>"},{"location":"instructor/#22-samplesheets-operators-and-groovy","title":"2.2 Samplesheets, operators, and groovy","text":"Tuples: grouping related dataReshaping data with operatorsDevelopment and debugging best practices <p>Tuples are introduced to group related pieces of information together (e.g. sample name + file paths) and prevent accidental mixing of file and metadata. At this stage, focus on the why, not the mechanics. </p> <p>More tuple usage will be visited in later lessons.</p> <p>Operators are often one of the hardest concepts for Nextflow beginners, but one of the most important for developing reproducible and scalable pipelines. They use Groovy syntax and it may not be obvious:</p> <ul> <li>Which operator to use</li> <li>When to use it</li> <li>Why the input shape needs to change</li> </ul> <p>Emphasise the why: operators exist to transform input data into the exact structure required by the next process.</p> <p>At this stage, learners do not need to memorise operators. they need to understand that:</p> <ul> <li>Processes expect inputs in the correct format</li> <li>Operators are the tool used to get data into those formats</li> </ul> <p>This section demonstrates only one example. Later lessons will showcase different requirements where different operators need to be used.</p> <p>Use this lesson to model real-world Nextflow development habits:</p> <ul> <li>Encourage frequent use of <code>.view()</code> to inspect channel outputs</li> <li>Encourage regular use of <code>-resume</code> to avoid re-running completed steps</li> <li>Normalise frequent and fast iteration when learners develop their own pipelines</li> </ul>"},{"location":"instructor/#23-multiple-process-inputs","title":"2.3 Multiple process inputs","text":"Accessing process outputsWhen inputs should be grouped together vs. separated <p>This lesson applies Lesson 1.7 to a bioinformatics use case. Recall that there are different valid ways to access outputs, such as:</p> <ul> <li>Adding <code>.out</code> to the end of a process name only works for single-element channels</li> <li>For single-element channels, using <code>.out[0]</code> is identical to <code>.out</code></li> <li><code>emit</code> can be used to name outputs</li> </ul> <p>In this lesson, learners extend their workflow by defining a process that accepts multiple inputs and chaining it to upstream processes. </p> <p>One thing to clarify is when processes should have inputs all in a single tuple, or as separate inputs.</p> <p>Emphasise to keep files, values, and metadata together in a tuple if they must stay together (e.g. sample ID, paired FASTQs).</p> <p>Inputs should be seperate if they can be shared across samples, or are constant across samples. For example, the reference index <code>transcriptome.fa</code> can be used across different paired FASTQs.</p> <p>Grouping everything as a tuple introduces unecessary data processing steps, while splitting incorrectly can result in unexpected behaviour, such as not all samples running.</p>"},{"location":"instructor/#24-combining-channels-multiple-process-outputs","title":"2.4 Combining channels, multiple process outputs","text":"<p>This lesson demonstrates how combining channels allows multiple process outputs to be collected and used by a single downstream process.</p> <p>MultiQC is the ideal example to demonstrate this as it requires all bioinformatics tool outputs to be aggregated and run in a single output.</p> <p>Note that the <code>input: path \"*\"</code> and <code>script: multiqc .</code> blocks follow the MultiQC Nextflow integration recommendations. </p> <p>State that this permissive pattern is generally bad practice. Communicate to learners that it is highly preferred to be as explicit the exact files and folders required. This improves error handling, testing, and future maintainability.</p>"},{"location":"instructor/#25-upscaling-to-multiple-samples-and-introspection","title":"2.5 Upscaling to multiple samples and introspection","text":"<p>This final lesson demonstrates how a workflow that works for a single sample can be scaled to many samples without modifying the pipeline code itself. Advise learners that scaling in Nextflow is primarily a configuration problem, not a coding problem. By updating the samplesheet, the same workflow logic is reused to run tasks in parallel across multiple samples.</p> <p>It is important to communicate that this section is only an introduction to configuration, benchmarking, and scaling concepts. Direct learners to the Nextflow on HPC workshop which covers these topics in detail, and was developed as a sequel to this introductory workshop.</p>"},{"location":"resources/","title":"Supporting materials","text":""},{"location":"resources/#glossary","title":"Glossary","text":"Term Definition Channels Asynchronous first-in, first-out (FIFO) queues used for communication between processes. They are the 'pipes' of the pipeline, defining how data flows between processes. Channel factories Special functions used to create channels. Common examples include <code>Channel.of()</code>, <code>Channel.fromPath()</code>, and <code>Channel.fromFilePairs()</code>. Containers Self-contained environments that package all the software and dependencies needed for each tool, ensuring consistency, reproducibility, and portability across different systems. Singularity containers are used. Directives Definition blocks that a process may contain, used to define settings like <code>publishDir</code>, <code>container</code>, or <code>tag</code>. Executor The component that determines how and where a process script is executed. Input Block A required block within a process definition used to define the input data for the process. Input definitions must include a qualifier (e.g., <code>val</code> or <code>path</code>) and a name. <code>main.nf</code> The core script that defines the steps of a Nextflow workflow. It outlines each process and how they are connected, focusing on what the workflow does. Nextflow A workflow orchestration engine and domain-specific language (DSL) that makes it easy to write data-intensive computational workflows. It defines complex program interactions and a high-level parallel computational environment based on the dataflow programming model. <code>nextflow.config</code> A key file for any Nextflow workflow that allows defining important settings and configurations that control how the workflow should run. It can be used to manage Singularity containers, resource usage (e.g., CPUs), and reporting. <code>output</code> block A block in a process definition used to define the output data the process is expected to produce. It requires both an output qualifier (e.g., <code>path</code> or <code>val</code>) and an output name. Parameters (<code>params</code>) Special values that can be set from command line arguments, allowing for flexible and dynamic pipelines. They are globally accessible by both processes and workflows and can be customized at runtime using a double hyphen flag (e.g., <code>--greeting</code>). Processes Small units that make up Nextflow workflows. A process describes a single step of the pipeline, including its inputs, expected outputs, and the code to run it. Processes execute independently as tasks and communicate via channels. <code>publishDir</code> directive A process directive used to specify where and how output files should be saved. This instructs Nextflow to copy all of the process' outputs to a specified directory, which helps organize results neatly. It is not recommended to use the directory created by this directive as an input to subsequent processes. Queue channels The more common type of channel that holds a series, or queue, of values and passes them into a process one at a time. The order of values in a queue channel is non-deterministic (First-In, First-Out or FIFO). Samplesheet A delimited text file (like a CSV) where each row contains information or metadata that needs to be processed together. Using samplesheets is considered best practice for reading grouped files and metadata into Nextflow. <code>script</code> block A required block within a process definition that contains the command and logic that will be executed (e.g., Bash, Python, or R code). <code>tag</code> directive A process directive that allows adding a custom label to each executed task. It is useful for identifying what is being run when executing workflows, especially with multiple samples, and for profiling. Tasks Independent units of execution for processes. When a task is created, Nextflow stages the input files, script, and other helper files into the task directory. Tuple An ordered collection of objects. When a tuple is used as input to a Nextflow process, it ensures that the objects inside are grouped and processed together as a single unit. Value channels Channels that store one and only one value, which can then be used multiple times. Work Directory (<code>work/</code>) The location where Nextflow runs all processes and stores their associated files. Every time a process is run, a directory inside <code>work/</code> is created with a unique randomly-generated ID. By default, all files created by processes exist only inside this directory. Workflow The overall composition of processes and dataflow logic. It is created by joining together processes and defines the logic that puts all of the processes together."},{"location":"resources/#recommended-resources","title":"Recommended resources","text":"<p>Here are some useful resources we recommend to help you get started with running nf-core pipelines and developing Nextflow pipelines:</p>"},{"location":"resources/#developed-by-us","title":"Developed by us","text":"<ul> <li>SIH Nextflow template</li> <li>SIH Nextflow template guide</li> <li>SIH Customising nf-core workshop</li> <li>Australian BioCommons Seqera Platform Service</li> <li>NCI Gadi nf-core instutitonal config</li> <li>Pawsey Setonix nf-core instutitional config</li> </ul>"},{"location":"resources/#developed-by-others","title":"Developed by others","text":"<ul> <li>Nextflow training</li> <li>Nextflow patterns</li> <li>Nextflow blog</li> <li>Nextflow coding best practice recommendations</li> <li>Seqera community forums</li> </ul>"},{"location":"resources/#nextflow-tips-and-tricks","title":"Nextflow tips and tricks","text":"<p>Nextflow has some useful features for executing pipelines and querying metadata and history. Here are some resources to help you get started.</p>"},{"location":"resources/#query-specific-pipeline-executions","title":"Query specific pipeline executions","text":"<p>The Nextflow log command is useful for querying execution metadata and history. You can filter your queries and output specific fields in the printed log.</p> <pre><code>nextflow log &lt;run_name&gt; -help\n</code></pre>"},{"location":"resources/#execute-nextflow-in-the-background","title":"Execute Nextflow in the background","text":"<p>The <code>-bg</code> options allows you to run your pipeline in the background and continue using your terminal. It is similar to <code>nohup</code>. You can redirect all standard output to a log file.</p> <pre><code>nextflow run &lt;workflow_repo/main.nf&gt; -bg &gt; workshop_tip.log\n</code></pre>"},{"location":"resources/#capture-a-nextflow-pipelines-configuration","title":"Capture a Nextflow pipeline's configuration","text":"<p>The Nextflow config command prints the resolved pipeline configuration. It is especially useful for printing all resolved parameters and profiles Nextflow will use to run a pipeline.</p> <pre><code>nextflow config &lt;workflow_repo&gt; -help\n</code></pre>"},{"location":"resources/#clean-nextflow-cache-and-work-directories","title":"Clean Nextflow cache and work directories","text":"<p>The Nextflow clean command will remove files from previous executions stored in the <code>.nextflow</code> cache and <code>work</code> directories. The <code>-dry-run</code> option allows you to preview which files will be deleted.</p> <pre><code>nextflow clean &lt;workflow_repo&gt; -help\n</code></pre>"},{"location":"resources/#change-default-nextflow-cache-strategy","title":"Change default Nextflow cache strategy","text":"<p>Workflow execution is sometimes not resumed as expected. The default behaviour of Nextflow cache keys is to index the input files meta-data information. Reducing the cache stringency to <code>lenient</code> means the files cache keys are based only on filesize and path, and can help to avoid unexpectedly re-running certain processes when <code>-resume</code> is in use.</p> <p>To apply lenient cache strategy to all of your runs, you could add to a custom configuration file:</p> <pre><code>process {\n    cache = 'lenient'\n}\n</code></pre> <p>You can specify different cache stategies for different processes by using <code>withName</code> or <code>withLabel</code>. You can specify a particular cache strategy be applied to certain <code>profiles</code> within your institutional config, or to apply to all profiles described within that config by placing the above <code>process</code> code block outside the <code>profiles</code> scope.</p>"},{"location":"resources/#access-private-github-repositories","title":"Access private GitHub repositories","text":"<p>To interact with private repositories on GitHub, you can provide Nextflow with access to GitHub by specifying your GitHub user name and a Personal Access Token in the <code>scm</code> configuration file inside your specified <code>.nextflow/</code> directory:</p> <pre><code>providers {\n\n  github {\n    user = 'georgiesamaha'\n    password = 'my-personal-access-token'\n  }\n\n}\n</code></pre>"},{"location":"setup/","title":"Setting up your computer","text":"<p>In this workshop, we will be using Virtual Machines (VM) on NCI's Nirin Cloud.</p> <p>The requirements for this workshop are a personal computer with:</p> <ul> <li>Visual Studio Code (VSCode)</li> <li>A web browser</li> </ul> <p>Below, you will find instructions on how to set up VSCode and connect to your VM. Each participant will be provided with their instances IP address prior to the workshop. Before the workshop, you must have the following:</p> <ol> <li>VSCode installed.</li> <li>The necessary VSCode extensions installed.</li> <li>Be able to connect to your VM.</li> </ol> <p>Info</p> <p>If you require assistance with the setup, please write in the discussion board on the Google document.</p>"},{"location":"setup/#installing-visual-studio-code","title":"Installing Visual Studio Code","text":"<p>Visual Studio Code (VSCode) is a versatile code editor that we will use for the workshop. We will use VSCode to connect to the VM, navigate the directories, edit, view and download files.</p> <ol> <li>Download VSCode by following the installation instructions for your local Operating System.</li> <li>Open VSCode to confirm it was installed correctly.</li> </ol> <p></p>"},{"location":"setup/#installing-the-vscode-extensions","title":"Installing the VSCode extensions","text":"<p>Specific VSCode extensions are required to connect to the VM and make working with Nextflow files easier (i.e. syntax highlighting).</p> <ol> <li>In the VSCode sidebar on the left, click on the extensions button (four blocks).</li> <li>In the Extensions Marketplace search bar, search for <code>remote ssh</code>. Select \"Remote - SSH\".</li> </ol> <p> 3. Click on the blue <code>Install</code> button.</p> <p> 4. Search for <code>nextflow</code> and install the \"Nextflow\" extension.</p> <p> 5. Close the Extensions tab and sidebar</p>"},{"location":"setup/#setting-up-your-remote-ssh-config","title":"Setting up your remote SSH config","text":"<ol> <li>In VSCode, press <code>Ctrl+Shift+P</code> (<code>Command+Shift+P</code> on mac) to open the Command Palette.</li> </ol> <p> 2. Type <code>remote ssh</code> and select <code>Remote-SSH: Add New SSH Host...</code>. This may appear in a different position in the list.</p> <p> 3. Enter the SSH connection string with the IP address that was provided to you. The connection string should look like: </p> <pre><code>ssh training@XXX.XXX.XX.XX\n</code></pre> <p>Ensure that you replace the <code>XXX.XXX.XX.XX</code> with your allocated IP address. Press <code>Enter</code>.</p> <p> 4. You will be prompted to <code>Select SSH configuration file to update</code>. Select your <code>.ssh/config</code> file.</p> <p> 5. You should receive a pop-up informing that a host as been added!</p>"},{"location":"setup/#connecting-to-the-vm","title":"Connecting to the VM","text":"<p>Ensure you have configured your SSH details.</p> <ol> <li> <p>In VSCode, press <code>Ctrl+Shift+P</code> (<code>Command+Shift+P</code> on mac) to open the Command Palette. Alternatively, you may use the search bar at the top of the VSCode window.</p> </li> <li> <p>Type <code>remote ssh</code> and select <code>Remote-SSH: Connect to Host...</code>. This may appear in a different position in the list.</p> </li> </ol> <p> 3. Select the IP address that you have configured. A new VSCode window will open.</p> <p>Connecting for the first time</p> <p>When you connect to your VM for the first time, you may need to configure a few settings:</p> <ul> <li>If you are prompted with <code>&lt;ip address&gt; has fingerprint ...</code>, Select <code>Continue</code></li> <li>If you are prompted for a platform, select <code>\"Linux\"</code></li> </ul> <ol> <li>Input your allocated password and hit 'Enter'.</li> </ol> <p></p> <ol> <li>Once the blue square in the bottom-left of the VSCode window shows <code>SSH: XXX.XXX.XX.XX</code> - you have successfully connected to your instance! <p></p>"},{"location":"setup/#configuring-vscode-for-the-workshop","title":"Configuring VSCode for the workshop","text":"<ol> <li>Select the File Explorer on the left sidebar (icon with two pages) or press <code>Ctrl+Shift+E</code> (Mac: <code>Cmd+Shift+E</code>).</li> </ol> <p> 2. Select <code>Open Folder</code></p> <p> 3. The correct file path should be input by default (<code>/home/training/</code>). Select <code>OK</code>.  4. If prompted, select \"Yes, I trust the authors\".   5. The home directory will appear in the left side bar.</p> <p> 6. In the Explorer sidebar, select the <code>part2</code> dropdown, then select <code>.main.nf</code>. This file will open in a tab. You may need to re-enter you password again.  7. Check that syntax highlighting (different parts of the Nextflow code are coloured) is enabled as shown. This is to confirm the VSCode Nextflow extension is working correctly.</p> <p></p> <p>Warning</p> <p>The <code>.main.nf</code> file is for testing purposes only. We will not touch this file in the workshop.</p> <ol> <li>Toggle the terminal in VSCode by pressing <code>Ctrl+j</code> (<code>Cmd+j</code> on mac).</li> </ol> <p></p> <p>Success</p> <p>You have now configured VSCode for the workshop!</p>"},{"location":"part1/00_whynextflow/","title":"1.0 Why Nextflow?","text":"<p>Learning objectives</p> <ol> <li>Describe the benefits of using Nextflow to write complex pipelines</li> <li>List community resources you can access to get help developing your pipelines</li> </ol> <p>Nextflow is a workflow orchestration engine and domain-specific language (DSL) that makes it easy to write data-intensive computational workflows.</p> <p>It is designed around the idea that the Linux platform is the lingua franca of data science. Linux provides many simple but powerful command-line and scripting tools that, when chained together, facilitate complex data manipulations.</p> <p>Nextflow extends this approach, adding the ability to define complex program interactions and a high-level parallel computational environment based on the dataflow programming model.</p>"},{"location":"part1/00_whynextflow/#101-core-features","title":"1.0.1 Core features","text":"<p>Nextflow\u2019s core features are:</p> <ul> <li>Workflow portability and reproducibility</li> <li>Scalability of parallelization and deployment</li> <li>Integration of existing tools, systems, and industry standards</li> </ul>"},{"location":"part1/00_whynextflow/#102-processes-tasks-and-channels","title":"1.0.2 Processes, tasks, and channels","text":"<p>A Nextflow workflow is made by joining together processes. Each process can be written in any scripting language that can be executed from the command line, such as Bash, Python, or R.</p> <p>Processes in are executed independently (i.e., they do not share a common writable state) as tasks and can run in parallel, allowing for efficient utilization of computing resources. Nextflow automatically manages the data dependencies between processes, ensuring that each process is executed only when its input data is available and all of its dependencies have been satisfied.</p> <p>The only way they can communicate is via asynchronous first-in, first-out (FIFO) queues, called channels. Simply, every input and output of a process is represented as a channel. The interaction between these processes, and ultimately the workflow execution flow itself, is implicitly defined by these input and output declarations.</p> <p></p>"},{"location":"part1/00_whynextflow/#103-execution-abstraction","title":"1.0.3 Execution abstraction","text":"<p>While a process defines what command or script is executed, the executor determines how and where the script is executed.</p> <p>Nextflow provides an abstraction between the workflow\u2019s functional logic and the underlying execution system. This abstraction allows users to define a workflow once and execute it on different computing platforms without having to modify the workflow definition. Nextflow provides a variety of built-in execution options, such as local execution, HPC cluster execution, and cloud-based execution, and allows users to easily switch between these options using command-line arguments.</p> <p></p>"},{"location":"part1/00_whynextflow/#104-more-information","title":"1.0.4 More information","text":"<p>This workshop focuses on basic skills for developers.</p> <p>Follow these links to find out more about Nextflow:</p> <ul> <li>Nextflow docs</li> <li>nf-core</li> <li>Nextflow training</li> <li>Reproducible workflows with nf-core</li> <li>Seqera community</li> </ul> <p>Summary</p> <p>In this step you have learned:</p> <ol> <li>How to describe different parts of a Nextflow pipeline</li> <li>How to find more information</li> </ol>"},{"location":"part1/01_intro/","title":"1.1 Welcome","text":"<p>In Part 1 of this workshop, you will create a toy multi-step Nextflow workflow.</p> <p>We will start Part 1 by familiarising ourselves with some common bash commands. Next, we will turn these commands into a small single-step Nextflow pipeline that will print a greeting to our terminal. In a series of exercises, we will then iterate on this pipeline to make it more flexible using variable outputs, inputs, and parameters. Finally, we will add a second step to the pipeline to dynamically turn our greeting into uppercase letters and name pipeline outputs.</p> <p>During Part 2, the skills and concepts you have learned in Part 1 will be applied in a more realistic scenario.</p>"},{"location":"part1/01_intro/#111-moving-into-your-work-directory","title":"1.1.1 Moving into your work directory","text":"<p>It is good practice to organise projects into their own folders to make it easier to track and replicate experiments over time. We have created separate directories for each part (<code>~/part1/</code> and <code>~/part2/</code>).</p> <p>Exercise</p> <p>In the VSCode terminal, move into the directory for all Part 1 activities:</p> <pre><code>cd ~/part1\n</code></pre>"},{"location":"part1/02_helloworld/","title":"1.2 Hello World!","text":"<p>Learning objectives</p> <ol> <li>Recall bash commands to manipulate strings (<code>echo</code>, <code>ls</code>, <code>cat</code>)</li> <li>Understand how output redirection works (<code>&gt;</code>)</li> </ol> <p>A Hello, World! is a minimalist example that is meant to demonstrate the basic syntax and structure of a programming language or software framework. The example typically consists of printing the phrase 'Hello World!' to the output, such as the console or terminal, or writing it to a file.</p> <p>Let's demonstrate this with simple commands that you can run directly in the terminal.</p>"},{"location":"part1/02_helloworld/#121-printing-a-string","title":"1.2.1 Printing a string","text":"<p>The <code>echo</code> command in Linux is a built-in command that allows users to display lines of text or strings that are passed as arguments. It is commonly used in shell scripts and batch files to output status text to the screen or a file.</p> <p>The most straightforward usage of the <code>echo</code> command is to display text or a string on the terminal. To do this, you simply provide the desired text or string as an argument to the <code>echo</code> command:</p> <pre><code>echo &lt;string&gt;\n</code></pre> <p>Exercise</p> <p>Use the <code>echo</code> command to print the string <code>'Hello World!'</code> to the terminal.</p> Solution <pre><code>echo 'Hello World!'\n</code></pre> Output<pre><code>Hello World!\n</code></pre>"},{"location":"part1/02_helloworld/#122-redirecting-outputs","title":"1.2.2 Redirecting outputs","text":"<p>The output of the <code>echo</code> can be redirected to a file instead of displaying it on the terminal. You can achieve this by using the <code>&gt;</code> operator for output redirection.</p> <p>Exercise</p> <p>Use the <code>&gt;</code> operator to redirect the output of echo to a file named <code>output.txt</code>.</p> Solution <pre><code>echo 'Hello World!' &gt; output.txt\n</code></pre> Output<pre><code>\n</code></pre> <p>Instead of printing the output to the terminal, this will write the output of the echo command to the file name <code>output.txt</code>.</p>"},{"location":"part1/02_helloworld/#123-listing-files","title":"1.2.3 Listing files","text":"<p>The Linux shell command <code>ls</code> lists directory contents of files and directories. It provides valuable information about files, directories, and their attributes.</p> <p><code>ls</code> will display the contents of the current directory:</p> <pre><code>ls\n</code></pre> <p>Exercise</p> <p>List the files in the working directory to verify <code>output.txt</code> was created.</p> Solution <pre><code>ls\n</code></pre> <p>A file named <code>output.txt</code> should now be listed in your current directory.</p> Output<pre><code>output.txt\n</code></pre>"},{"location":"part1/02_helloworld/#124-viewing-file-contents","title":"1.2.4 Viewing file contents","text":"<p>The <code>cat</code> command in Linux is a versatile companion for various file-related operations, allowing users to view, concatenate, create, copy, merge, and manipulate file contents.</p> <p>The most basic use of <code>cat</code> is to display the contents of a file on the terminal. This can be achieved by simply providing the filename as an argument:</p> <pre><code>cat &lt;file name&gt;\n</code></pre> <p>Exercise</p> <p>Use the <code>cat</code> command to print the contents of <code>output.txt</code>.</p> Solution <pre><code>cat output.txt\n</code></pre> <p>You should see <code>Hello World!</code> printed to your terminal.</p> Output<pre><code>Hello World!\n</code></pre> <p>In preparation for the following lessons, delete <code>output.txt</code>.</p> <pre><code>rm output.txt\n</code></pre> <p>Summary</p> <p>This lesson recalls basic bash commands using a classic \"Hello World!\" example, including:</p> <ol> <li>Printing to the terminal with <code>echo</code></li> <li>Redirecting output to a file with <code>&gt;</code></li> <li>Viewing files and file contents with <code>ls</code> and <code>cat</code></li> </ol>"},{"location":"part1/03_hellonf/","title":"1.3 Writing your first pipeline","text":"<p>Learning objectives</p> <ol> <li>Understand the structure and purpose of <code>process</code> and <code>workflow</code> blocks in Nextflow scripts</li> <li>Understand how to define a simple <code>process</code> using <code>script</code> and <code>output</code> blocks</li> <li>Create a complete, single-step Nextflow pipeline</li> <li>Evaluate script readability using comments</li> </ol> <p>Workflow languages, such as Nextflow, provide a structured way of managing multi-step analyses. Workflow languages can help you coordinate individual tasks, handle dependencies, enable parallel execution, and improve reproducibility. They also make your code easier to maintain and share with others.</p> <p>Here, you're going to learn more about the Nextflow language and take your first steps making your first pipeline with Nextflow.</p>"},{"location":"part1/03_hellonf/#131-understanding-the-process-and-workflow-scopes","title":"1.3.1 Understanding the <code>process</code> and <code>workflow</code> scopes","text":"<p>Nextflow pipelines are written inside <code>.nf</code> files. They consist of a combination of two main components: processes and the workflow itself. Each process describes a single step of the pipeline, including its inputs and expected outputs, as well as the code to run it. The workflow then defines the logic that puts all of the processes together.</p> <p></p>"},{"location":"part1/03_hellonf/#process","title":"<code>process</code>","text":"<p>A process definition starts with the keyword <code>process</code>, followed by a process name, and finally the process body delimited by curly braces. The process body must contain a <code>script</code> block. The <code>script</code> block contains the command and logic that will be executed.</p> <p>A process may contain any of the following definition blocks. The ones we will be focusing on this workshop are presented in bold: <code>directives</code>, <code>input</code>, <code>output</code>, <code>stub</code>, <code>when</code> clauses, and of course, <code>script</code>.</p> <pre><code>process &lt; name &gt; {\n  [ directives ]\n\n  input:\n    &lt; process inputs &gt;\n\n  output:\n    &lt; process outputs &gt;\n\n  script:\n  \"\"\"\n  &lt;script to be executed&gt;\n  \"\"\"\n}\n</code></pre>"},{"location":"part1/03_hellonf/#workflow","title":"<code>workflow</code>","text":"<p>A workflow is a composition of processes and dataflow logic.</p> <p>The workflow definition starts with the keyword <code>workflow</code>, followed by an optional name, and finally the workflow body delimited by curly braces.</p> <pre><code>workflow {\n    &lt; processes to be executed &gt;\n}\n</code></pre> <p>Tip</p> <p>The <code>process</code> and <code>workflow</code> definitions are analogous to functions in R or Python languages. You first have to define a function (the <code>process</code>) that contains the instructions of what to do. In order to do the action, the function needs to be called (in the <code>workflow</code>).</p> <p>Let's review the structure of <code>hello-world.nf</code>, a toy example you will be developing and executing in Part 1.</p> hello-world.nf<pre><code>process SAYHELLO {\n\n    output:\n    path 'output.txt'\n\n    script:\n    \"\"\"\n    echo 'Hello World!' &gt; output.txt\n    \"\"\"\n}\n\nworkflow {\n    SAYHELLO()\n}\n</code></pre> <p>The first piece of code (lines 1-11) describes a process called <code>SAYHELLO</code> with two definition blocks:</p> <ul> <li>output: defines that the process will output a file called <code>output.txt</code>. It also contains the <code>path</code> qualifier. We will review this in the next section.</li> <li>script: the <code>echo 'Hello World!'</code> command redirects to a file called <code>output.txt</code></li> </ul> <p>The second block of code (12-14) lines describes the workflow itself, which consists of one call to the <code>SAYHELLO</code> process.</p> <p>We will start building the <code>hello-world.nf</code> script, piece-by-piece, so you can get a feel for the requirements for a writing a Nextflow workflow.</p> <p>The order of steps taken to build the process in the following exercises are intentional. We will be building processes by defining each process block starting with the <code>script</code> then the <code>output</code>. In later sections, we will add <code>input</code> and process <code>directives</code> to this order.</p> <p>This order is not prescriptive, however, the <code>script</code> logic often determines how the other process blocks should look like and this order can be helpful for breaking down building processes in a logical way. This approach will be continued in Part 2, when you build an RNAseq workflow!</p> <p>Exercises</p> <ol> <li> <p>Create a new file <code>hello-world.nf</code>.</p> </li> <li> <p>In the new file, define an empty <code>process</code> and call it <code>SAYHELLO</code>.</p> </li> </ol> Solution hello-world.nf<pre><code>process SAYHELLO {\n\n}\n</code></pre> <p><ol> <li>Add the <code>script</code> definition that writes 'Hello World!' to a file called <code>output.txt</code>.</li> </ol></p> Solution hello-world.nf<pre><code>process SAYHELLO {\n\n    script:\n    \"\"\"\n    echo 'Hello World!' &gt; output.txt\n    \"\"\"\n}\n</code></pre>"},{"location":"part1/03_hellonf/#132-commenting-your-code","title":"1.3.2 Commenting your code","text":"<p>Before we complete our process, we will comment our code. Commenting your code is important so we, and others, can easily understand what the code is doing (you will thank yourself later).</p> <p>In Nextflow, a single line comment can be added by prepending it with two forward slash (<code>//</code>):</p> <pre><code>// This is my comment\n</code></pre> <p>Similarly, multi-line comments can be added using the following format:</p> <pre><code>/*\n *  This is my\n *  multi-line\n *  comment\n */\n</code></pre> <p>As a developer you can to choose how and where to comment your code.</p> <p>Exercise</p> <p>Add a comment to the pipeline to describe what the process block is doing. You can use the comments provided in the solutions, however we highly recommend  writing your own comment that is useful for your understanding.</p> Solution <p>The solution may look something like this:</p> hello-world.nf<pre><code>/*\n * Use echo to print 'Hello World!'\n * to a text file\n */\nprocess SAYHELLO {\n\n    script:\n    \"\"\"\n    echo 'Hello World!' &gt; output.txt\n    \"\"\"\n}\n</code></pre> <p>Or this:</p> hello-world.nf<pre><code>// Use echo to print 'Hello World!' to a text file\nprocess SAYHELLO {\n\n    script:\n    \"\"\"\n    echo 'Hello World!' &gt; output.txt\n    \"\"\"\n}\n</code></pre> <p>As a developer, you get to choose!</p>"},{"location":"part1/03_hellonf/#133-capturing-process-outputs","title":"1.3.3 Capturing process outputs","text":"<p>In the previous section, you have defined the <code>script</code> - what the <code>SAYHELLO</code> process should do. We also need to tell Nextflow to expect this output file - otherwise, it will ignore it!</p> <p>We declare outputs using the <code>output</code> definition block. Typically this will require both an output qualifier and an output name:</p> <pre><code>output:\n&lt;output qualifier&gt; &lt;output name&gt;\n</code></pre> <p>Output qualifiers are keywords used inside processes to tell Nextflow what type of output to expect (e.g. a folder, file, or value).</p> <p>Common output qualifiers include <code>path</code> and <code>val</code>:</p> <ul> <li><code>path</code>: Emit a file produced by the process with the specified name</li> <li><code>val</code>: Emit the variable with the specified name</li> </ul>"},{"location":"part1/03_hellonf/#path","title":"<code>path</code>","text":"<p>For example, the syntax <code>path \"output.txt\"</code> tells Nextflow that the process outputs a file called <code>output.txt</code>. Nextflow can then track that file and pass it correctly to any downstream processes that need it.</p>"},{"location":"part1/03_hellonf/#val","title":"<code>val</code>","text":"<p>If you attempt to use the output <code>val \"output.txt\"</code>, it will outputs the literal string <code>\"output.txt\"</code>. Any downstream process that expects a file won't be able to do anything with this and will fail. The <code>val</code> output is useful for keeping track of metadata across steps. We will explore this more in Part 2.</p> <p>See the Nextflow documentation for a full list of output qualifiers.</p> <p>Warning</p> <p>Qualifiers don't only indicate the type of file, but it's an important part of telling Nextflow processes how this information should be handled. If you set the wrong qualifier, the pipeline will likely throw errors.</p> <p>The output name is a name given to the output variable. This can be whatever you want it to be, however, the output name and the file generated by the script must match, or else Nextflow won't find it and will throw an error.</p> <p>Note</p> <p>It's important to remember that the output block doesn't create the output, it just tells Nextflow what to expect after the process runs. Whether or not that output actually exists depends on what your script does inside the <code>script</code> block. If your script doesn't create the file, Nextflow won't find it and will assume the process has failed. This is helpful because:</p> <ul> <li>Nextflow checks that the declared output is actually there. If it's missing it knows something went wrong;</li> <li>Nextflow uses outputs to connect processes together, it waits for the output before moving to the next step.</li> </ul> <p>Exercise</p> <p>In your <code>SAYHELLO</code> process, add an <code>output</code> block that captures <code>'output.txt'</code>. Since it is a file being emitted by the process, the <code>path</code> qualifier must be used.</p> Solution hello-world.nf<pre><code>// Use echo to print 'Hello World!' to a text file\nprocess SAYHELLO {\n\n    output:\n    path 'output.txt'\n\n    script:\n    \"\"\"\n    echo 'Hello World!' &gt; output.txt\n    \"\"\"\n}\n</code></pre> <p>This example is brittle because the output filename is hardcoded in two separate places (the <code>script</code> and the <code>output</code> definition blocks). If you change one but not the other, the script will break because they need to match. We will review how to solve this in the Dynamic Naming lesson.</p> <p>In addition, this process does not require an input block as the contents are predefined in the script block logic. We will explore input blocks in the following lessons.</p> <p>You have now defined your first process!</p>"},{"location":"part1/03_hellonf/#134-calling-the-process-in-the-workflow-scope","title":"1.3.4 Calling the process in the <code>workflow</code> scope","text":"<p>We have now defined a functional process for <code>SAYHELLO</code>. To ensure that it runs, you need to call the process in the <code>workflow</code> scope. Recall that the process is analagous to a function that we need to instruct to run in <code>workflow</code>.</p> <p>Exercise</p> <ol> <li>At the bottom of your <code>main.nf</code> script, after the <code>SAYHELLO</code> process, define an empty <code>workflow</code> scope:</li> </ol> Solution hello-world.nf<pre><code>// Use echo to print 'Hello World!' and redirect to output.txt\nprocess SAYHELLO {\n\n    output:\n    path 'output.txt'\n\n    script:\n    \"\"\"\n    echo 'Hello World!' &gt; output.txt\n    \"\"\"\n}\n\nworkflow {\n\n}\n</code></pre> <p><ol> <li>Add the process call by adding <code>SAYHELLO()</code> inside <code>workflow {}</code>.</li> </ol></p> Solution hello-world.nf<pre><code>// Use echo to print 'Hello World!' and redirect to output.txt\nprocess SAYHELLO {\n\n    output:\n    path 'output.txt'\n\n    script:\n    \"\"\"\n    echo 'Hello World!' &gt; output.txt\n    \"\"\"\n}\n\nworkflow {\n    // Emit a greeting\n    SAYHELLO()\n}\n</code></pre> <p>Note, that we did not include any arguments to <code>SAYHELLO()</code>. This is because the process does not take any inputs. </p> <p>Yay! You have just written your first pipeline!</p> <p>In the next step, we will run the pipeline and inspect the outputs of our workflow.</p> <p>Summary</p> <p>This lesson introduced the Nextflow scripting language and walked you through:</p> <ol> <li>Defining a simple <code>process</code> using <code>script</code> and <code>output</code> blocks</li> <li>Understanding how the <code>workflow</code> scope will trigger execution</li> <li>Adding single or multi-line comments</li> <li>Using output qualifiers like <code>path</code> vs. <code>val</code></li> <li>Understanding the consistency between the output declaration and the actual files produced</li> </ol>"},{"location":"part1/04_execution/","title":"1.4 Running your first pipeline","text":"<p>Learning objectives</p> <ol> <li>Execute a basic Nextflow pipeline using <code>nextflow run</code></li> <li>Understand the outputs and logs generated by a pipeline run</li> <li>Understand task caching and the use of the <code>-resume</code> flag</li> <li>Apply the <code>publishDir</code> process directive to organise outputs</li> <li>Evaluate the usefulness of publishing outputs vs relying on work directory navigation</li> </ol> <p>In this step, we will run our <code>hello-world.nf</code> Nextflow pipeline and explore the outputs of the run. We will look at the components that get printed to the terminal when executing a workflow,  how to interpret these, as well as the common log and output files of a run. You will be introduced to your first process directive and best practices on managing output files.</p>"},{"location":"part1/04_execution/#141-executing-hello-worldnf","title":"1.4.1 Executing <code>hello-world.nf</code>","text":"<p>To run a Nextflow pipeline we use the <code>nextflow run</code> command, followed by the name of the script.</p> <pre><code>nextflow run &lt;pipeline.nf&gt;\n</code></pre> <p>Let's run the <code>.nf</code> script we just created - remember to save the file first!</p> <p>Exercise</p> <p>Use the <code>nextflow run</code> command to execute <code>hello-world.nf</code></p> Solution <pre><code>nextflow run hello-world.nf\n</code></pre> <p>Yay! You have just run your first pipeline!</p> <p>Your console should look something like this:</p> <pre><code>N E X T F L O W  ~  version 24.10.2\nLaunching `hello-world.nf` [mighty_murdock] DSL2 - revision: 80e92a677c\nexecutor &gt;  local (1)\n[4e/6ba912] SAYHELLO [100%] 1 of 1 \u2714\n</code></pre> <p>What does each line mean?</p> <ol> <li>The version of Nextflow that was executed</li> <li>The script and version names</li> <li>The executor used (in the above case: local)</li> <li>The process is executed once, which means there is one task. The line starts with a unique hexadecimal value, and ends with the task completion information</li> </ol> <p>Currently it is not obvious where our <code>output.txt</code> file has been written to.</p>"},{"location":"part1/04_execution/#142-understanding-the-work-and-task-directories","title":"1.4.2 Understanding the work and task directories","text":"<p>When a task is created, Nextflow stages the task input files, script, and other helper files into the task directory. The task writes any output files to this directory during its execution, and Nextflow uses these output files for downstream tasks and/or publishing.</p> <p>These directories do not share a writable state, and any required files or information must be passed through channels (this will be important later).</p> <p>Warning</p> <p>The work directory might not have the same hash as the one shown above.</p> <p>Let's inspect the work directory.</p> <p>Exercises</p> <ol> <li> <p>In the terminal, run <code>ls</code> to view the files in the directory.</p> Solution <p>Terminal<pre><code>ls\n</code></pre> Output<pre><code>hello-world.nf  output.txt  work \n</code></pre></p> <p>Running our <code>hello-world.nf</code> pipeline created a new directory called <code>work</code>. Note that <code>output.txt</code> was not from the pipeline we just ran, but from the exercises from lesson 1.2.</p> </li> <li> <p>Inspect the <code>work</code> directory by running <code>tree -a work</code> in the terminal.</p> Solution <p><code>tree</code> shows you the file and directory structure of <code>work</code>. The <code>-a</code> flag includes hidden files (files that start with a <code>.</code>).</p> <p>Terminal<pre><code>tree -a work\n</code></pre> Output<pre><code>work/\n\u2514\u2500\u2500 4e\n    \u2514\u2500\u2500 6ba9138vhsbcbsc83bcka\n        \u251c\u2500\u2500 .command.begin\n        \u251c\u2500\u2500 .command.err\n        \u251c\u2500\u2500 .command.log\n        \u251c\u2500\u2500 .command.out\n        \u251c\u2500\u2500 .command.run\n        \u251c\u2500\u2500 .command.sh\n        \u251c\u2500\u2500 .exitcode\n        \u2514\u2500\u2500 output.txt\n</code></pre></p> </li> </ol> <p>A series of log files and any outputs are created by each task in the work directory:</p> <ul> <li><code>.command.begin</code>: Metadata related to the beginning of the execution of the process task</li> <li><code>.command.err</code>: Error messages (stderr) emitted by the process task</li> <li><code>.command.log</code>: Complete log output emitted by the process task</li> <li><code>.command.out</code>: Regular output (<code>stdout</code>) by the process task</li> <li><code>.command.sh</code>: The command that was run by the process task call</li> <li><code>.exitcode</code>: The exit code resulting from the command</li> </ul> <p>These files are created by Nextflow to manage the execution of your pipeline. While these file are not required now, you may need to interrogate them to troubleshoot issues later.</p> <p>Note that our <code>output.txt</code> file created by the <code>SAYHELLO</code> process is also in the same task directory.</p> <p>Exercise</p> <p>View the <code>.command.sh</code> file</p> Solution <p>Note: The hash may be different to the example shown below.</p> <p><pre><code>cat work/4e/6ba9138vhsbcbsc83bcka/.command.sh\n</code></pre> Output<pre><code>#!/bin/bash -ue\necho 'Hello World!' &gt; output.txt\n</code></pre></p> <p>The <code>.command.sh</code> is the bash script that Nextflow creates and runs for the <code>SAYHELLO</code> process defined in <code>hello-world.nf</code>. In this example it shows the same <code>script</code> block as the process. Inspecting <code>.command.sh</code> is very useful for troubleshooting once you introduce parameters and dynamic naming, when it is not as clear how the <code>script</code> block will look like.</p>"},{"location":"part1/04_execution/#143-caching-tasks-and-resuming-workflows","title":"1.4.3 Caching tasks and resuming workflows","text":"<p>One of the core features of Nextflow is the ability to store task executions (caching). These cached tasks and files can be reused by Nextflow to minimise duplicating work, and let's you resume pipelines. </p> <p>Instead of having to run the entire pipeline from the beginning, you can tell Nextflow to run only the processes that errored. This is extremely useful for iteratively developing a pipeline.</p> <p>Note</p> <p>Each time a task runs, Nextflow creates a unique task directory inside the <code>work/</code> directory.  The generated hash ensures that each task can be uniquely identified. This is important for checkpointing, especially when you can be running thousands of tasks in a single pipeline. The hash is computed from different metadata such as your compute environment and some details of the process. More information can be found in the Nextflow docs on task hash.</p> <p>In the next exercise, we will run our <code>hello-world.nf</code> with the <code>-resume</code> flag and review how caching allows resumability.</p> <p>Question</p> <p>Run the command <code>nextflow run hello-world.nf -resume</code>.</p> Solution <pre><code>N E X T F L O W  ~  version 24.10.2\nLaunching `hello-world.nf` [mighty_murdock] DSL2 - revision: 80e92a677c\nexecutor &gt;  local (1)\n[4e/6ba912] SAYHELLO [100%] 1 of 1, cached: 1 \u2714\n</code></pre> <p>The output you receive is the same as the first time the pipeline was ran, with the addition of <code>cached: 1</code>. The workflow was executed from the beginning, however, before running the task, Nextflow used the unique task ID to check if the task directory already exists and was completed succesfully or not.</p> <p>Since we already ran the <code>SAYHELLO</code> task, it completed without error, and the task directory with the matching unique ID exists, these previous results are used as the process results.</p> <p>Note</p> <p>The <code>-resume</code> flag was used in the step to demonstrate Nextflow's caching feature.  This will be used extensively later in Part 2.</p>"},{"location":"part1/04_execution/#144-publishing-outputs","title":"1.4.4 Publishing outputs","text":"<p>By default, all files created by processes exist only inside the <code>work</code> directory. When we have pipelines with multiple processes that generate many output files, it is not feasible to view each task directory for each of our output files.</p> <p>To make our outputs more accessible and neatly organised, we define a publishing strategy, which determines which outputs should be copied to a final publishing directory.</p> <p>The <code>publishDir</code> directive can be used to specify where and how output files should be saved. For example:</p> <pre><code>publishDir 'results'\n</code></pre> <p>By adding the above to a process, all output files would be saved in a new folder called <code>results</code> in the current directory, in addition to the work directory. The <code>publishDir</code> directive is process specific.</p> <p>Exercise</p> <ol> <li>Add <code>publishDir 'results'</code> in the <code>SAYHELLO</code> process block. </li> </ol> Solution hello-world.nf<pre><code>// Use echo to print 'Hello World!' and redirect to output.txt\nprocess SAYHELLO {\n    publishDir 'results'\n\n    output:\n    path 'output.txt'\n\n    script:\n    \"\"\"\n    echo 'Hello World!' &gt; output.txt\n    \"\"\"\n}\n</code></pre> <p><ol> <li>Execute the pipeline again. View your new <code>results</code> folder in the working directory.</li> </ol></p> <p>Do not use <code>publishDir</code> as an input into processes</p> <p>Recall that <code>output</code> definitions tells Nextflow when to run the next process and ensure that the process ran successfully. The <code>publishDir</code> directive does not allow for these checks, and is a way to make results more findable after  the pipeline has finished running. We will revisit this in more detail in the next step.</p> <p>Summary</p> <p>This lesson focused on executing and understanding the runtime behaviour of a basic Nextflow pipeline, including:</p> <ol> <li>Running the pipeline and interpretting the terminal output</li> <li>Inspecting the work directory and task logs (e.g. <code>.command.sh</code>)</li> <li>Awareness of task hashing and caching for resumability</li> <li>Using <code>publishDir</code> to organise pipeline outputs</li> </ol>"},{"location":"part1/05_inputs/","title":"1.5 Inputs and Channels","text":"<p>Learning objectives</p> <ol> <li>Describe the differences between Nextflow channel types</li> <li>Apply input blocks in process definitions</li> <li>Create channels using channel factories such as <code>Channel.of()</code></li> <li>Implement process calls that take single and multiple inputs via channels</li> <li>Evaluate correct vs incorrect input strategies (e.g. why not to use <code>publishDir</code> as an input)</li> </ol> <p>So far, you've been emitting a text string ('Hello World!') that has been hardcoded into the script block. In a more realistic situation, you might want to pass a variable input to your script, much like you pass files to command line tools for analysis.</p> <p>Here you're going to to add some flexibility by introducing channels to your workflow and an input definition to your <code>SAYHELLO</code> process.</p>"},{"location":"part1/05_inputs/#151-passing-information-through-channels","title":"1.5.1 Passing information through channels","text":"<p>In Nextflow, processes primarily communicate through channels. Channels are essentially the 'pipes' of our pipeline, providing a way for data to flow between our processes and defining the overall structure of the workflow.</p> <p>Channels allow us to handle inputs efficiently by defining which data should be taken from one step to another. Channels are one of Nextflows key features that allow us to run jobs in parallel, as well as many other benefits.</p> <p></p> <p>This image illustrates a few use cases of channels that we will implement:</p> <ul> <li>1 shows our current <code>hello-world.nf</code> pipeline where the <code>SAYHELLO</code> process always prints a file called <code>output.txt</code> that contains <code>Hello World!</code></li> <li>2 shows how you can provide a text input to a process, via a channel, to change what is redirected into <code>output.txt</code>. In this example, <code>'Hello World!'</code> is replaced with <code>'Bonjour'</code>.</li> <li>3 shows how to use the output of a process (<code>SAYHELLO</code>) as the input of another process. The <code>CONVERTUPPER</code> process replaces all lower case characters in <code>output.txt</code> to uppercase, and saves it to a different file called <code>upper.txt</code>.</li> </ul> <p>There are two kinds of channels: queue channels and value channels.</p>"},{"location":"part1/05_inputs/#queue-channels","title":"Queue channels","text":"<p>Queue channels are the more common type of channel. It holds a series, or queue, of values and passes them into a process one at a time. Values you can expect in a queue channel can be file paths, or values such as strings and numbers. Generally, a queue channel contains values of the same type.</p> <p>One important behaviour of queue channels is that the order of the values is non-deterministic. You will not know ahead of time the order of values within the queue due to it's 'first-in, first-out' (FIFO) nature. More on this in the following sections.</p> <p>A value in a queue channel can only be used (run) once in a process.</p>"},{"location":"part1/05_inputs/#value-channels","title":"Value channels","text":"<p>Value channels, as their name suggests, simply store a value. Importantly:</p> <ul> <li>Value channels can be bound (i.e. assigned) with one and only one value.</li> <li>The assigned value can be used multiple times.</li> </ul> <p>When a value channel is passed as an input to a process, its value will be used for every run of that process.</p>"},{"location":"part1/05_inputs/#152-creating-channels","title":"1.5.2 Creating channels","text":"<p>Channels are created in one of two ways. The first is as outputs of processes. Each entry in the <code>output</code> block of a process creates a separate channel that can be accessed with <code>&lt;process_name&gt;.out</code> - or, in the case of named outputs, with <code>&lt;process_name&gt;.out.output_name</code>.</p> <p>The other way to create channels is with special functions called channel factories. There are numerous types of channel factories which can be utilised for creating different channel types and data types. The most common channel factories you will use are:</p> <ul> <li><code>Channel.of()</code></li> <li><code>Channel.fromPath()</code></li> <li><code>Channel.fromFilePairs()</code>.</li> </ul> <p>The latter two are fairly self explanatory, creating channels of file paths and pairs of file paths, respectively. The <code>Channel.of()</code> factory is a much more generic method used to create a channel of whatever values are passed to it.</p> <p>For example, an exercise we will be working through is to create a channel called <code>greeting_ch</code> that contains three values - 'Hello World!', 'Bonjour le monde!', 'Hol\u00e0 mundo':</p> <pre><code>greeting_ch = Channel.of('Hello world!', 'Bonjour le monde!', 'Hol\u00e0 mundo')\n</code></pre> <p>A process consuming this channel would run three times - once for each value.</p>"},{"location":"part1/05_inputs/#153-adding-channels-to-our-pipeline","title":"1.5.3 Adding channels to our pipeline","text":"<p>You're going to start by creating a channel with the <code>Channel.of()</code> channel factory that will contain your greeting.</p> <p>Note</p> <p>You can build different kinds of channels depending on the shape of the input data.</p> <p>Channels need to be created within the <code>workflow</code> definition.</p> <p>Exercise</p> <p>Create a channel named <code>greeting_ch</code> with the 'Hello World!' greeting. Create <code>greeting_ch</code> before the process that runs it.</p> Solution hello-world.nf<pre><code>workflow {\n\n    // Create a channel for inputs\n    greeting_ch = Channel.of('Hello world!')\n\n    // Emit a greeting\n    SAYHELLO()\n}\n</code></pre>"},{"location":"part1/05_inputs/#154-adding-channels-as-process-inputs","title":"1.5.4 Adding channels as process inputs","text":"<p>Before <code>greeting_ch</code> can be passed to the <code>SAYHELLO</code> process as an input, you must first add an input block in the process definition.</p> <p>The inputs in the input block, much like the output block, must have a qualifier and a name:</p> <pre><code>input:\n&lt;input qualifier&gt; &lt;input name&gt;\n</code></pre> <p>Input names can be treated like a variable, and while the name is arbitrary, it should be recognisable.</p> <p>No quote marks are needed for variable inputs. For example:</p> <pre><code>input:\nval greeting\n</code></pre> <p>Similar to the output qualifiers discussed in the previous chapter, there are several different input qualifiers, with some of the more common ones being:</p> <ul> <li><code>val</code>: A value, such as a string or number.</li> <li><code>path</code>: A file path.</li> </ul> <p>Exercise</p> <p>Add an <code>input</code> block to the <code>SAYHELLO</code> process with an input value. Update the comment at the same time.</p> Solution hello-world.nf<pre><code>// Use echo to print a string and redirect to output.txt\nprocess SAYHELLO {\n    publishDir 'results'\n\n    input:\n    val greeting\n\n    output:\n    path 'output.txt'\n\n    script:\n    \"\"\"\n    echo 'Hello World!' &gt; output.txt\n    \"\"\"\n}\n</code></pre> <p>The <code>SAYHELLO</code> process is now expecting an input value.</p> <p>The <code>greeting_ch</code> channel can now be supplied to the <code>SAYHELLO()</code> process within the workflow block:</p> <pre><code>SAYHELLO(greeting_ch)\n</code></pre> <p>Without this, Nextflow will throw an error.</p> <p>Exercise</p> <p>Add the <code>greeting_ch</code> as an input for the <code>SAYHELLO</code> process.</p> Solution hello-world.nf<pre><code>workflow {\n\n    // Create a channel for inputs\n    greeting_ch = Channel.of('Hello world!')\n\n    // Emit a greeting\n    SAYHELLO(greeting_ch)\n}\n</code></pre>"},{"location":"part1/05_inputs/#155-using-nextflow-variables-within-scripts","title":"1.5.5 Using Nextflow variables within scripts","text":"<p>The final piece is to update the <code>script</code> block to use the <code>input</code> value.</p> <p>Each input can be accessed as a variable via the name in its definition. Within the script block, this is done by prepending a <code>$</code> character to the input name:</p> <pre><code>script:\n\"\"\"\necho '$greeting' &gt; output.txt\n\"\"\"\n</code></pre> <p>The <code>'</code> quotes around <code>$greeting</code> are required by the <code>echo</code> command to treat the greeting as a single string.</p> <p>Exercises</p> <ol> <li>Update <code>hello-world.nf</code> to use the greeting input.</li> </ol> Solution hello-world.nf<pre><code>// Use echo to print 'Hello World!' and redirect to output.txt\nprocess SAYHELLO {\n    publishDir 'results'\n\n    input:\n    val greeting\n\n    output:\n    path 'output.txt'\n\n    script:\n    \"\"\"\n    echo '$greeting' &gt; output.txt\n    \"\"\"\n}\n\nworkflow {\n\n    // Create a channel for inputs\n    greeting_ch = Channel.of('Hello world!')\n\n    // Emit a greeting\n    SAYHELLO(greeting_ch)\n}\n</code></pre> <p><ol> <li> Save, and run the pipeline (<code>nextflow run hello-world.nf</code>). Inspect <code>results/output.txt</code>.</li> </ol></p> Solution <p><pre><code>cat results/output.txt\n</code></pre> Output.txt<pre><code>Hello World! \n</code></pre></p> <p>The process will still function the same and produce the same output, but instead takes the values in <code>greeting_ch</code> (<code>'Hello World!'</code>) as input.</p> <p>Note</p> <p>Similar to the <code>output</code> block in a process, the <code>input</code> does not determine the <code>input</code> of process. Recall that it simply declares what input should be expected based on the logic in side the <code>script</code> block. </p> <p>Yes! Your pipeline now uses an input channel!</p>"},{"location":"part1/05_inputs/#156-running-processes-on-multiple-inputs","title":"1.5.6 Running processes on multiple inputs","text":"<p>Now that we have a channel set up and our process has been reworked to use it, we can very easily start feeding more inputs into the channel and watch <code>SAYHELLO</code> run on each one.</p> <p>The <code>Channel.of()</code> factory can take any number of values, separated by commas. Each one will become a separate element in the queue channel. Any process consuming that channel will run once for every element; each run will be separate and in parallel to the rest.</p> <p>Exercise</p> <p>Add additional greetings to <code>greeting_ch</code>.</p> Solution hello-world.nf<pre><code>workflow {\n\n    // Create a channel for inputs\n    greeting_ch = Channel.of('Hello world!', 'Bonjour le monde!', 'Hol\u00e0 mundo')\n\n    // Emit a greeting\n    SAYHELLO(greeting_ch)\n}\n</code></pre> <p>If you now run the workflow again, you should see that <code>SAYHELLO</code> runs three times:</p> <pre><code>Launching `main.nf` [curious_hugle] DSL2 - revision: 243f7816c2\n\nexecutor &gt;  local (3)\n[27/ed09aa] SAYHELLO (1) [100%] 3 of 3 \u2714\n</code></pre> <p>Notice that by default Nextflow only prints out one line per process rather than one line per run of each process. Sometimes it may be useful to you to have it print out each run on a separate line. To do this, you can add the <code>-ansi-log false</code> flag to the command line:</p> <pre><code>nextflow run hello-world.nf -ansi-log false\n</code></pre> <p>The output now looks like:</p> <pre><code>Launching `main.nf` [deadly_wilson] DSL2 - revision: 243f7816c2\n[f2/84d334] Submitted process &gt; SAYHELLO (2)\n[f4/9f72e1] Submitted process &gt; SAYHELLO (1)\n[dc/52fa3d] Submitted process &gt; SAYHELLO (3)\n</code></pre> <p>There is only one <code>output.txt</code> in our <code>results/</code> folder. This is because we have hardcoded the output name. Each time the process is run, it overwrites the existing <code>output.txt</code> and the one we see is from the last process that was run.</p> <p>Note</p> <p>If you inspect your <code>output.txt</code>, it can contain any one of the greetings specified. This is because the order that tasks are run are non-deterministic. Nextflow tasks run on a FIFO (first-in, first-out) basis, meaning that a task will begin running once it is ready. Different factors can impact this, but a common scenario is when  the output of a process is ready, the downstream process that uses it as input will be exectuted.</p>"},{"location":"part1/05_inputs/#157-a-note-about-multiple-input-channels","title":"1.5.7 A note about multiple input channels","text":"<p>The input block can be used to define multiple inputs to the process. Importantly, the number of inputs passed to the process call within the workflow must match the number of inputs defined in the process. For example:</p> example.nf<pre><code>process MYFUNCTION {\n    publishDir 'results'\n\n    input:\n    val input_1\n    val input_2\n\n    output:\n    path 'output.txt'\n\n    script:\n    \"\"\"\n    echo $input_1 $input_2 &gt; output.txt\n    \"\"\"\n}\n\nworkflow {\n    MYFUNCTION('Hello', 'World!')\n}\n</code></pre> <p>If we view the file:</p> <p><pre><code>cat output.txt\n</code></pre> Output<pre><code>Hello World!\n</code></pre></p> <p>The main caveat when using multiple input channels is that the order of values and files can be unpredictable (non-deterministic). The best practice here is to apply a combination of operators to combine the multiple channels. Nextflow's processes documentation has an overview of how it works, and recommendations when it should be used in your own pipelines.</p>"},{"location":"part1/05_inputs/#158-faq-can-i-use-the-publishdir-as-an-input-to-a-process","title":"1.5.8 FAQ: Can I use the <code>publishDir</code> as an input to a process?","text":"<p>A common question about Nextflow is whether the <code>publishDir</code> can be used as an input to processes. This can sometimes seem like an attractive and useful pattern. For example, you may have several processes generating outputs that need to get collated or summarised in some way at the end of your workflow. If each process puts its final output in the <code>publishDir</code> directory, then the final summary process can simply look there for all its inputs.</p> <p>Unfortunately, this is not good practice, and will very likely either lead to inconsistent results or not work at all. <code>publishDir</code> is not really part of the workflow itself; it's more like a \"side branch\" to the pipeline that acts as a convenient place to put important outputs of the workflow. There are no guarantees about when output files will get copied into <code>publishDir</code>, and Nextflow won't know to wait for files to be generated inside before running processes that rely on them.</p> <p></p> <p>Ultimately, all processes should be working with channels as their inputs, and <code>publishDir</code> should only be used as a final output directory.</p> <p>Summary</p> <p>Channels are one of Nextflow's most powerful features by allowing data to flow into, and between processes. In this lesson, we:</p> <ol> <li>Created a queue channel</li> <li>Used the <code>val</code> input qualifer to provide a string input</li> <li>Modified the <code>SAYHELLO()</code> process to use a channel instead of hardcoded strings</li> <li>Ran a process with multiple values</li> <li>Learnt the risks of using <code>publishDir</code> as a source of input (not recommended)</li> </ol>"},{"location":"part1/06_params/","title":"1.6 Parameters","text":"<p>Learning objectives</p> <ol> <li>Understand how parameters enable flexible and configurable pipelines</li> <li>Apply command-line flags to define parameters during execution</li> <li>Modify pipeline behaviour dynamically using <code>params</code> within process and workflow scopes</li> <li>Evaluate when to use parameters for outputs to support reproducibility and avoid overwrites</li> </ol> <p>Parameters are special values that can be set from command line arguments and therefore allow you to write flexible and dynamic pipelines. Parameters in Nextflow are comparable to \"variables\" in other programming languages (e.g. R, Python).</p> <p>Here you're going to update the script with parameters to make it more flexible.</p>"},{"location":"part1/06_params/#161-why-are-parameters-useful","title":"1.6.1 Why are parameters useful?","text":"<p>Parameters are useful because they can be set with a convenient default value in a script but can then be overwritten at runtime using a flag. Simply, parameters allow us to configure some aspect of a pipeline without editing the script itself.</p> <p>Nextflow has multiple levels of configuration and, as different levels may have conflicting settings, they are ranked in order of priority and some configuration can be overridden.</p> <p>Parameters can be created within the top level of your Nextflow script (i.e. outside of the <code>workflow</code> and <code>process</code> definitions) by prefixing a parameter name with the <code>params</code> scope (e.g. <code>params.greeting</code>). They are globally accessible by both processes and workflows anywhere in your workflow. They can be modified when you run your pipeline by adding a double hyphen (<code>--</code>) to the start of the parameter name (e.g. <code>--greeting</code>) and adding it to an execution command:</p> <pre><code>nextflow run hello-world.nf --greeting 'Hello World!'\n</code></pre>"},{"location":"part1/06_params/#162-updating-your-workflow-with-the-greeting-parameter","title":"1.6.2 Updating your workflow with the <code>--greeting</code> parameter","text":"<p>Instead of hard coding 'Hello World!' as an input, a parameter, with a default value, can be created:</p> <pre><code>params.greeting = 'Hello World!'\n</code></pre> <p>Parameters must begin with <code>params.</code>, and the name the follows directly can be of your choosing. In this case, the name is <code>greeting</code>.</p> <p>The parameter can then be used in a channel factory (just like the hard coded string):</p> <pre><code>greeting_ch = Channel.of(params.greeting)\n</code></pre> <p>The parameter can then be flexibly changed using a <code>--greeting</code> flag in the run command:</p> <pre><code>nextflow run hello-world.nf --greeting 'Bonjour le monde!'\n</code></pre> <p>Warning</p> <p>When setting parameters from the command line, it is vital that you wrap multi-word strings (or anything else containing spaces or special characters) within single or double quotes as shown above. If you don't, only the first word will be captured in the parameter. Anything after a space will be considered to be a new argument to Nextflow and behave unexpectedly.</p> <p>Exercise</p> <p>Update the <code>hello-world.nf</code> script to use a <code>greeting</code> parameter as an input. Define the default for the <code>greeting</code> parameter at the top of the script and give it the default value <code>'Hello World!'</code>.</p> Solution hello-world.nf<pre><code>// Set default greeting\nparams.greeting = 'Hello World!'\n\n// Use echo to print 'Hello World!' and redirect to output.txt\nprocess SAYHELLO {\n    publishDir 'results'\n\n    input:\n    val greeting\n\n    output:\n    path 'output.txt'\n\n    script:\n    \"\"\"\n    echo '$greeting' &gt; output.txt\n    \"\"\"\n}\n\nworkflow {\n\n    // Create a channel for inputs\n    greeting_ch = Channel.of(params.greeting)\n\n    // Emit a greeting\n    SAYHELLO(greeting_ch)\n\n}\n</code></pre> <p>The <code>hello-world.nf</code> pipeline can now be executed with the <code>--greeting</code> flag and a custom greeting:</p> <pre><code>nextflow run hello-world.nf --greeting 'Bonjour le monde!'\n</code></pre>"},{"location":"part1/06_params/#163-setting-a-dynamic-publishing-directory-with-outdir","title":"1.6.3 Setting a dynamic publishing directory with <code>--outdir</code>","text":"<p>Hardcoding a single output directory in your pipeline is often inconvenient - each new run can overwrite the previous results. By using a parameter for the <code>publishDir</code> process directive, it makes our pipeline more configurable, and results more reproducible:</p> <pre><code>publishDir params.outdir\n</code></pre> <p>To ensure this works even if a parameter is not provided, we can set default values for <code>publishDir</code>:</p> <pre><code>params.outdir = 'results'\n</code></pre> <p>However, you may consider having no default values for parameters and letting the pipeline fail to prevent the accidental overwriting of results.</p> <p>This can be especially useful when you want to:</p> <ul> <li>Keep outputs from each run separate</li> <li>Name output folders based on the input data or command being run</li> <li>You are iteratively testing a pipeline and want a clear record of each run</li> </ul> <p>Exercises</p> <ol> <li>Update the <code>hello-world.nf</code> script to use an <code>outdir</code> parameter as the publishing directory. Define the default for the <code>outdir</code> parameter at the top of the script and give it the default value <code>'results'</code>.</li> </ol> Solution hello-world.nf<pre><code>// Set default greeting\nparams.greeting = 'Hello World!'\n\n// Set default output directory\nparams.outdir = 'results'\n\n// Use echo to print 'Hello World!' and redirect to output.txt\nprocess SAYHELLO {\n    publishDir params.outdir\n\n    input:\n    val greeting\n\n    output:\n    path 'output.txt'\n\n    script:\n    \"\"\"\n    echo '$greeting' &gt; output.txt\n    \"\"\"\n}\n\nworkflow {\n\n    // Create a channel for inputs\n    greeting_ch = Channel.of(params.greeting)\n\n    // Emit a greeting\n    SAYHELLO(greeting_ch)\n\n}\n</code></pre> <p><ol> <li>Re-run the pipeline with <code>nextflow run hello-world.nf</code>. Include different values to your new <code>--greeting</code> and <code>--outdir</code> parameters and inspect the outputs. What happens if you do not include any flags? i.e. <code>nextflow run hello-world.nf</code></li> </ol></p> Solution <p>Running without any flags will use the default parameters specified in <code>hello-world.nf</code>: <pre><code>nextflow run hello-world.nf\n</code></pre></p> <p>You can pass any string into either parameter. In this case, it will redirect 'Good day, Earth!' into a file <code>output.txt</code>, and publish that file into a folder called <code>gday_earth</code>: <pre><code>nextflow run hello-world.nf --greeting 'Good day, Earth!' --outdir 'gday_earth'\n</code></pre></p> <p>To illustrate how we can clearly name and separate runs using parameters: <pre><code>nextflow run hello-world.nf --greeting 'Bonjour le monde' --outdir 'bonjour'\n</code></pre></p> <p>Don't forget to wrap strings with spaces or special characters within single or double quotes!</p> <p>Summary</p> <p>In this lesson, we updated our pipelines to support flexible execution using parameters:</p> <ol> <li>Declared global parameter values for the greeing message and output directory</li> <li>Used parameterised inputs with <code>Channel.of(params.greeting)</code></li> <li>Applied the <code>--greeting</code> and <code>--outdir</code> flags to override defaults at runtime</li> <li>Replaced hardcoded values with configurable options</li> <li>Recognised the benefits of configuring output names for reproducibility</li> </ol>"},{"location":"part1/07_process/","title":"1.7 Adding new processes","text":"<p>Learning objectives</p> <ol> <li>Revise the use of piping and text transformation in bash (<code>tr</code>, <code>cat</code>, <code>|</code>)</li> <li>Implement a new <code>process</code> block to perform transformations on text files</li> <li>Implement connecting the output of a process as an input of another process via channels</li> <li>Evaluate process outputs and how they interact in a multi-step pipeline</li> </ol> <p>Up until now you've been modifying a single step. However, pipelines generally consist of multiple steps where outputs from one step are used as inputs for the next.</p> <p>Here you're going to step things up again and add another process to the pipeline.</p>"},{"location":"part1/07_process/#171-translating-text","title":"1.7.1 Translating text","text":"<p>The <code>tr</code> command is a UNIX command-line utility for translating or deleting characters. It supports a range of transformations including uppercase to lowercase, squeezing repeating characters, deleting specific characters, and basic find and replace. It can be used with UNIX pipes to support more complex translation. <code>tr</code> stands for translate. The following example will translate all lower case letters (represented by the pattern <code>[a-z]</code>) to upper case (represented by <code>[A-Z]</code>):</p> <pre><code>tr '[a-z]' '[A-Z]'\n</code></pre> Advanced content: regular expressions <p>For the curious, the patterns we are using here - e.g. <code>[a-z]</code> - are called regular expressions. They are a way of describing patterns in text and can be immensely useful in manipulating text as they provide a way to search and replace text in more complex ways than simple exact matches. Be warned, they can get very complicated and confusing very quickly!</p>"},{"location":"part1/07_process/#172-piping-commands","title":"1.7.2 Piping commands","text":"<p>The pipe command in Linux, represented by the vertical bar symbol <code>|</code>, is an essential tool for command-line enthusiasts and professionals alike. The primary purpose of the pipe command is to connect the output of one command directly into the input of another:</p> <pre><code>echo 'Hello World' | tr '[a-z]' '[A-Z]'\n</code></pre> <p>The contents of a file can be piped using the <code>cat</code> command:</p> <pre><code>cat output.txt | tr '[a-z]' '[A-Z]'\n</code></pre> <p>Like before, the output can be redirected to an output file:</p> <pre><code>cat output.txt | tr '[a-z]' '[A-Z]' &gt; upper.txt\n</code></pre>"},{"location":"part1/07_process/#173-adding-the-converttoupper-process","title":"1.7.3 Adding the <code>CONVERTTOUPPER</code> process","text":"<p>The output of the <code>SAYHELLO</code> process is a text file called <code>output.txt</code>.</p> <p>In the next step of the pipeline, you will add a new process named <code>CONVERTTOUPPER</code> that will convert all of the lower case letters in this file to a uppercase letters and save them as a new file.</p> <p>The <code>CONVERTTOUPPER</code> process will follow the same structure as the <code>SAYHELLO</code> process:</p> <pre><code>process CONVERTTOUPPER {\n    publishDir params.outdir\n\n    input:\n    &lt;input qualifier&gt; &lt;input name&gt;\n\n    output:\n    &lt;output qualifier&gt; &lt;output name&gt;\n\n    script:\n    \"\"\"\n    &lt;script&gt;\n    \"\"\"\n}\n</code></pre> <p>Using what you have learned in the previous sections you will now write a new process using the <code>tr</code> command from above.</p> <p>Exercise</p> <p>Add new process named <code>CONVERTTOUPPER</code> that will take an input text file, convert all of the lowercase letters in the text file to uppercase letters, and save a new text file that contains the translated letters.</p> <p>The <code>CONVERTOTUPPER</code> process should be defined inbetween your <code>SAYHELLO</code> process and <code>workflow</code> scope.</p> <p>Hints have been provided to aid you in writing the process. We recommend following the order of the hints, first defining the <code>script</code> block, followed by the <code>output</code> and <code>input</code>.</p> Hint: <code>script:</code> <p>The script might look something like this:</p> <pre><code>cat $input_file | tr '[a-z]' '[A-Z]' &gt; upper.txt\n</code></pre> <p>Hint 1: The input name is <code>input_file</code>, however, you may call it something different. Hint 2: The output text file is named <code>upper.txt</code></p> Hint: <code>output:</code> <p>The output</p> <pre><code>path 'upper.txt'\n</code></pre> <p>Hint 1: The output is a file and requires the <code>path</code> qualifier.</p> <p>Hint 2: The output name is hard coded as 'upper.txt', however, you may call it something different.</p> Hint: <code>input:</code> <pre><code>path input_file\n</code></pre> <p>Hint 1: The input is a file and requires the <code>path</code> qualifier.</p> <p>Hint 2: <code>input_file</code> must be the same as what was specified as the input name in the <code>script</code> block.</p> Solution hello-world.nf<pre><code>// Set default greeting\nparams.greeting = 'Hello World!'\n\n// Set a default output directory\nparams.outdir = 'results'\n\n// Use echo to print 'Hello World!' and redirect to output.txt\nprocess SAYHELLO {\n    publishDir params.outdir\n\n    input:\n    val greeting\n\n    output:\n    path 'output.txt'\n\n    script:\n    \"\"\"\n    echo '$greeting' &gt; output.txt\n    \"\"\"\n}\n\nprocess CONVERTTOUPPER {\n    publishDir params.outdir\n\n    input:\n    path input_file\n\n    output:\n    path 'upper.txt'\n\n    script:\n    \"\"\"\n    cat $input_file | tr '[a-z]' '[A-Z]' &gt; upper.txt\n    \"\"\"\n}\n\nworkflow {\n\n    // Create a channel for inputs\n    greeting_ch = Channel.of(params.greeting)\n\n    // Emit a greeting\n    SAYHELLO(greeting_ch)\n\n}\n</code></pre>"},{"location":"part1/07_process/#174-connecting-the-processes","title":"1.7.4 Connecting the processes","text":"<p>As we learned in the inputs module, Nextflow uses channels to connect processes. Each output defined in a process' <code>output</code> block defines a new channel that can be used as inputs for another process.</p> <p>The output channel from a process can be accessed by adding <code>.out</code> to the end of a process name in the workflow definition:</p> <pre><code>SAYHELLO.out\n</code></pre> <p>Outputs can then be used as an input for another process:</p> <pre><code>CONVERTTOUPPER(SAYHELLO.out)\n</code></pre> <p>Alternatively, you can assign the output channel to a new variable name for convenience:</p> <pre><code>hello_msg = SAYHELLO.out\n\nCONVERTTOUPPER(hello_msg)\n</code></pre> <p>The process output behaves like any other channel and can be used as inputs for multiple downstream processes.</p> <p>Warning</p> <p>Adding <code>.out</code> to the end of a process name only works for single outputs. If there are multiple outputs, you will need to use an integer index to select the appropriate output (e.g. <code>.out[0]</code> or <code>.out[1]</code> for the first and second inputs, respectively), or (more conveniently) use the <code>emit</code> option when defining the <code>output</code> block of the process, which allows you to select the output by name (e.g. <code>.out.some_output</code>). See the additional options section of the Nextflow documentation for more information.</p> <p>Exercise</p> <p>Add the <code>CONVERTTOUPPER</code> process to your workflow definition. Use the output from <code>SAYHELLO</code> as its input.</p> Solution hello-world.nf<pre><code>// Set default greeting\nparams.greeting = 'Hello World!'\n\n// Set a default output directory\nparams.outdir = 'results'\n\n// Use echo to print 'Hello World!' and redirect to output.txt\nprocess SAYHELLO {\n    publishDir params.outdir\n\n    input:\n    val greeting\n\n    output:\n    path 'output.txt'\n\n    script:\n    \"\"\"\n    echo '$greeting' &gt; output.txt\n    \"\"\"\n}\n\nprocess CONVERTTOUPPER {\n    publishDir params.outdir\n\n    input:\n        path input_file\n\n    output:\n        path 'upper.txt'\n\n    script:\n    \"\"\"\n    cat $input_file | tr '[a-z]' '[A-Z]' &gt; upper.txt\n    \"\"\"\n}\n\nworkflow {\n\n    // Create a channel for inputs\n    greeting_ch = Channel.of(params.greeting)\n\n    // Emit a greeting\n    SAYHELLO(greeting_ch)\n\n    // Convert the greeting to uppercase\n    CONVERTTOUPPER(SAYHELLO.out)\n\n}\n</code></pre> <p>Executing <code>hello-world.nf</code> will now show a second step:</p> <pre><code>N E X T F L O W  ~  version 23.10.1\nLaunching `hello-world.nf` [mighty_murdock] DSL2 - revision: 80e92a677c\nexecutor &gt;  local (2)\n[ef/b99a2f] SAYHELLO (1)       [100%] 1 of 1 \u2714\n[cd/c8cf1b] CONVERTTOUPPER (1) [100%] 1 of 1 \u2714\n</code></pre> <p>Summary</p> <p>In this section we extended our pipeline by introducing a second process, and connecting to the first. We:</p> <ol> <li>Explore the <code>tr</code> command for translating text to uppercase</li> <li>Defined a new process <code>CONVERTTOUPPER</code> to take a file and generate a modified output</li> <li>Connected the output of the <code>SAYHELLO</code> process to <code>CONVERTTOUPPER</code> using <code>.out</code></li> <li>Updated our workflow scope to call both processes sequentially</li> </ol>"},{"location":"part1/08_dynamic/","title":"1.8 Dynamic naming","text":"<p>Learning objectives</p> <ol> <li>Understand the purpose of dynamic output naming</li> <li>Apply input variables to dynamically generate output file names</li> <li>Distinguish between single and double quotes when working with variables in Groovy syntax</li> </ol> <p>When we run bioinformatics pipelines, it is common to process and identify results by the sample name or by other metadata.</p> <p>Currently, the outputs of the <code>SAYHELLO</code> and <code>CONVERTTOUPPER</code> processes are being saved as <code>output.txt</code> and <code>upper.txt</code>, respectively. In some situations this would be fine. However, it would help to identify the outputs by naming these dynamically.</p> <p>Let's get tricky and name your output files dynamically.</p>"},{"location":"part1/08_dynamic/#181-expressing-process-outputs-dynamically","title":"1.8.1 Expressing process outputs dynamically","text":"<p>When an output file name needs to be expressed dynamically, it is possible to define it using a dynamic string that references values defined in the input declaration block or in the script global context.</p> <p>For example, the <code>SAYHELLO</code> input value <code>greeting</code> can be used to help name the output file.</p> <pre><code>process SAYHELLO {\n    publishDir 'results'\n\n    input:\n    val greeting\n\n    output:\n    path \"${greeting}.txt\"\n\n    script:\n    \"\"\"\n    echo '$greeting' &gt; ${greeting}.txt\n    \"\"\"\n}\n</code></pre> <p>Curly brackets <code>{}</code> have been used to wrap <code>greeting</code> in the <code>output</code> and <code>script</code> block so it is interpreted as a variable as a part of a file name.</p> <p>There is an important difference between single-quoted (<code>'</code>) and double-quoted (<code>\"</code>)strings. Double-quoted strings support variable interpolations while single-quoted strings do not.</p> <p>Exercise</p> <p>Update the <code>SAYHELLO</code> and <code>CONVERTTOUPPER</code> process to use dynamic output names.</p> <p>Warning</p> <p>It's difficult to name a file with a space. Use a simple greeting without spaces, e.g., \"Hello\", when testing your pipeline.</p> Solution hello-world.nf<pre><code>// Use echo to print 'Hello World!' and redirect to output.txt\nprocess SAYHELLO {\n    publishDir 'results'\n\n    input:\n    val greeting\n\n    output:\n    path \"${greeting}.txt\"\n\n    script:\n    \"\"\"\n    echo '$greeting' &gt; ${greeting}.txt\n    \"\"\"\n}\n\n// Use tr to convert lowercase letters to upper case letters and save as upper.txt\nprocess CONVERTTOUPPER {\n    publishDir 'results'\n\n    input:\n        path input_file\n\n    output:\n        path \"upper_${input_file}\"\n\n    script:\n    \"\"\"\n    cat $input_file | tr '[a-z]' '[A-Z]' &gt; upper_${input_file}\n    \"\"\"\n}\n\nworkflow {\n\n    // Set default greeting\n    params.greeting = \"Hello\"\n\n    // Create a channel for inputs\n    greeting_ch = Channel.of(params.greeting)\n\n    // Emit a greeting\n    SAYHELLO(greeting_ch)\n\n    // Convert the greeting to uppercase\n    CONVERTTOUPPER(SAYHELLO.out)\n\n}\n</code></pre> <p>Let's execute your pipeline and view the changes to see if your outputs have been named dynamically.</p> <pre><code>nextflow run hello-world.nf --greeting 'Hello'\n</code></pre> <p>While the output will look the same:</p> <pre><code>N E X T F L O W  ~  version 24.10.2\nLaunching `hello-world.nf` [mighty_murdock] DSL2 - revision: 80e92a677c\nexecutor &gt;  local (2)\n[ef/b99a2f] SAYHELLO (1)       [100%] 1 of 1 \u2714\n[cd/c8cf1b] CONVERTTOUPPER (1) [100%] 1 of 1 \u2714\n</code></pre> <p>You should now see some new files in your results folder:</p> <ul> <li><code>Hello.txt</code></li> <li><code>upper_Hello.txt</code></li> </ul> <p>Summary</p> <p>In this lesson, we made our pipeline outputs more informative by:</p> <ol> <li>Using dynamic names to name outputs such as <code>Hello.txt</code></li> <li>Updated both the <code>SAYHELLO</code> and <code>CONVERTTOUPPER</code> processes to use interpolated file names</li> <li>Applied double-quoted strings and curly braces <code>{}</code> for variable expansion in both script and output blocks</li> <li>Noted the limitation of using file names with spaces, reinforcing the importance of simple input values for filenames</li> </ol>"},{"location":"part1/09_outro/","title":"1.9 Wrapping up","text":"<p>Congratulations on completing Part 1 of the Nextflow for the Life Sciences workshop!</p> <p>Throughout each lesson, you've built up a fully functional, and flexible Nextflow workflow from scratch. Along the way, you've gained practical experience with the key concepts and syntax that form the backbone of Nextflow pipelines.</p>"},{"location":"part1/09_outro/#191-key-takeaways","title":"1.9.1 Key takeaways","text":"<ul> <li>Nextflow lets you to chain together data processing steps that allow you to make complex bioinformatics workflows</li> <li>Nextflow workflows are built from small units (processes) that communicate via channels</li> <li>A variety of execution logs and outputs lets you troubleshoot your workflows</li> <li>Work and task directories are uniquely identified to enable resumability and checkpointing</li> <li>Channels are a powerful feature that lets you structure your workflows and provides the ability to run multiple tasks in parallel</li> <li>Using parameters can make your pipeline flexible and configurable upon runtime - no more hardcoding and editing your files everytime new data or samples are run</li> </ul>"},{"location":"part1/09_outro/#192-whats-next","title":"1.9.2 What's next?","text":"<p>In Part 1 we developed a simple \"Hello World!\" example to introduce the key Nextflow concepts required to build more complex, real-world bioinformatics workflows.</p> <p>Part 2 will continue by taking the concepts and examples introduced here and apply them to develop a simple RNAseq processing pipeline.</p>"},{"location":"part2/00_intro/","title":"2.0 Introduction","text":"<p>Part 2 builds on the fundamental concepts learned in Part 1 and provides you with  hands-on experience in Nextflow workflow development. Throughout the session  we will be working with a bulk RNAseq dataset to build our workflow. </p> <p>We will construct channels that control how our data flows through  processes that we will progressively construct to build our workflow.  Each lesson in Part 2 will build on the previous lessons, so you can gain a  deeper understanding of the techniques and the impact they have on your  resulting workflow. </p> <p>We will do this within the context of a real-world scenario: creating a multi-sample  Nextflow workflow for preparing RNAseq data. We will build the workflow  step-by-step, starting with a series of provided bash scripts and gradually  converting each into small workflow components.</p> <p>Along the way, you will encounter Nextflow concepts (both from Part 1 and some new ones) and our best practice recommendations for developing your own pipelines.  </p> <p>Part 2 of this workshop is based off the  Simple RNAseq workflow training material developed by Seqera.  </p>"},{"location":"part2/00_intro/#201-log-back-into-your-instance","title":"2.0.1 Log back into your instance","text":"<p>Re-connect to your Virtual Machine by following the  \"Connect to the VM\" section from the setup page.</p> <p>Once connected, in your VSCode terminal, change directories into the <code>part2/</code> directory:  </p> <pre><code>cd ~/part2/\n</code></pre> <p>All Part 2 activities will be conducted in this folder. </p>"},{"location":"part2/00_intro/#202-introducing-our-scenario-from-bash-scripts-to-scalable-workflows","title":"2.0.2 Introducing our scenario: from bash scripts to scalable workflows","text":"<p>Imagine you are a bioinformatician in a busy research lab. Your team will be receiving a large batch of samples that need to be processed through a series of analysis steps.  </p> <p>You have inherited a set of bash scripts from a former colleague, which were used to process a handful of samples manually. These scripts are robust and well-tested, but they were not designed with scalability in mind.  </p> <p>As more samples come in, running these scripts one by one is becoming increasingly tedious and error-prone.  </p> <p>You need a way to automate this process, ensuring consistency and efficiency across many samples. </p> <p>You decide to use Nextflow.  </p> <p>Exercise</p> <p>View the bash scripts your colleague provided:</p> <ol> <li>Use either the VSCode File Explorer, or the integrated terminal to navgiate  to the <code>~/part2/bash_scripts/</code> directory.</li> <li>Inspect the scripts (open in a VSCode tab or a text editor in the terminal).</li> </ol> <p>Each script runs a single data processing step and are run in order of the prefixed number.</p> <p>Poll</p> <p>What are some limitations of these scripts in terms of running them in a pipeline and monitoring it?  </p>"},{"location":"part2/00_intro/#203-our-workflow-rnaseq-data-processing","title":"2.0.3 Our workflow: RNAseq data processing","text":"<p>Don't worry if you don't have prior knowledge of RNAseq!</p> <p>The focus of this workshop is on learning Nextflow, the RNAseq data we  are using in this part are just a practical example to help you understand  how the workflow system works. </p> <p>RNAseq is used to study gene expression and has many applications across biomedicine, agriculture and evolutionary studies. In our scenario we are going to  run through some basic core steps that allow us to explore different aspects of  Nextflow. </p>"},{"location":"part2/00_intro/#the-data","title":"The data","text":"<p>The data we will use includes:</p> <ul> <li><code>*.fq</code>: Paired-end RNAseq reads in FASTQ format from three different samples (gut, liver, lung).  </li> <li><code>transcriptome.fa</code>: A transcriptome file in FASTA format.  </li> <li><code>samplesheet*.csv</code>: CSV files that help us track which files belong to which samples.</li> </ul>"},{"location":"part2/00_intro/#our-bioinformatics-tools","title":"Our bioinformatics tools","text":"<p>We will be implementing and integrating three commonly used bioinformatics tools:  </p> <ol> <li>Salmon is a tool for quantifying molecules known as transcripts through RNAseq data.  </li> <li>FastQC is a tool for quality analysis of high throughput sequence data. You can think of it as a way to assess the quality of your data.  </li> <li>MultiQC searches a given directory for analysis logs and compiles an HTML report for easy viewing. It's a general use tool, perfect for summarising the output from numerous bioinformatics tools.  </li> </ol> <p>These tools will be run using Singularity containers. We will not explore how the data and tools work further, and focus on how they should be implemented in a Nextflow workflow.  </p>"},{"location":"part2/00_intro/#204-pipeline-structure-and-design","title":"2.0.4 Pipeline structure and design","text":"<p>Having reviewed the bash scripts, we've decided to keep its modular structure and will build the following four processes (discrete steps):</p> <ol> <li><code>INDEX</code> - Transcriptome indexing (tool: Salmon): create an index of the reference transcriptome for faster and efficient data processing.</li> <li><code>FASTQC</code> - Raw data quality control (tool: FastQC): assess the quality of the FASTQ files to ensure our data is usable. </li> <li><code>QUANTIFICATION</code> - Gene quantification (tool: Salmon): counting how many reads map to each gene in the transcriptome. </li> <li><code>MULTIQC</code> - Summarise results in a report (tool: MultiQC): generate a report that summarises quality control and gene quantification results. </li> </ol> <p></p>"},{"location":"part2/00_intro/#205-nextflowing-the-workflow","title":"2.0.5 Nextflowing the workflow","text":"<p>Each lesson in part 2 of our workshop focuses on implementing one process of  the workflow at a time. We will iteratively build the workflow and processes  in a single <code>main.nf</code> file and lightly use a <code>nextflow.config</code> file for configuration.</p> <p></p>"},{"location":"part2/00_intro/#mainnf","title":"<code>main.nf</code>","text":"<p>The <code>main.nf</code> file is the core script that defines the steps of your Nextflow workflow. It outlines each <code>process</code> (which house the individual commands or data processing steps) and how they are connected to each other. This <code>main.nf</code> script focuses on what the workflow does.</p> <p>Most of the code you will write in Part 2 will go in <code>main.nf</code>. </p> <p>We will follow an ordered approach for each step of the workflow  building off the <code>process</code> structure from Part 1.3. You will be using this process template for each step of the workflow, adding  them to the <code>main.nf</code> script: </p> <pre><code>process &lt; name &gt; {\n  [ directives ]\n\n  input:\n    &lt; process inputs &gt;\n\n  output:\n    &lt; process outputs &gt;\n\n  script:\n  \"\"\"\n  &lt; script to be executed &gt;\n  \"\"\"\n}\n</code></pre>"},{"location":"part2/00_intro/#nextflowconfig","title":"<code>nextflow.config</code>","text":"<p>The <code>nextflow.config</code> file is a key part of any Nextflow workflow. While <code>main.nf</code> outlines the steps and processes of the workflow,  <code>nextflow.config</code> allows you to define important settings and configurations  that control how your workflow should run.</p> <p>This script will be intermittently used in the following lessons to control the use of Singularity containers, how much resources (e.g. CPUs) should be used, and reporting of the workflow after it has finished running.</p>"},{"location":"part2/01_salmon_idx/","title":"2.1 Implementing a simple process with a container","text":"<p>Learning objectives</p> <ol> <li>Implement a Nextflow process that takes a single file as input.  </li> <li>Understand the importance of containers in ensuring consistent and reproducible execution across processes.</li> <li>Apply the <code>publishDir</code> directive to store process outputs in a specified directory. </li> </ol> <p>In this lesson we will be implement <code>00_index.sh</code> as our first Nextflow process, <code>INDEX</code>. Here, we are working with the first step of the RNAseq data processing workflow: indexing the transcriptome for downstream processes. To do this, we will need to run Salmon's indexing mode.    Open the bash script <code>00_index.sh</code>:  </p> 00_index.sh<pre><code>mkdir \"results\"\nsalmon index \\\n    --transcripts data/ggal/transcriptome.fa \\\n    --index results/salmon_index\n</code></pre> <ul> <li>The script first creates a <code>results/</code> folder then runs the <code>salmon index</code> command.  </li> <li>The Salmon <code>--transcripts</code> flag indicates that the path to the input transcriptome file is <code>data/ggal/transcriptome.fa</code>.  </li> <li>The Salmon <code>--index results/salmon_index</code> flag tells <code>salmon</code> to save the output index files in a directory called <code>salmon_index</code>, within the newly created <code>results</code> directory.  </li> </ul> <p>Avoid hardcoding arguments by using parameters</p> <p>The paths to the transcriptome file (<code>data/ggal/transcriptome.fa</code>) and the output directory (<code>results/salmon_index</code>) are hardcoded in this bash script. If you wanted to change the input transcriptome file or the output location, you must manually edit the script. This makes our scripts less flexible and easy to use. </p> <p>As we will see, Nextflow addresses the issue of hardcoded paths by allowing values to be passed dynamically at runtime as parameters (<code>params</code>). </p>"},{"location":"part2/01_salmon_idx/#211-building-the-index-process","title":"2.1.1 Building the <code>INDEX</code> process","text":"<p>In the empty <code>main.nf</code> script, add the following <code>process</code> scaffold with the script definition:  </p> main.nf<pre><code>process INDEX {\n  [ directives ]\n\n  input:\n    &lt; process inputs &gt;\n\n  output:\n    &lt; process outputs &gt;\n\n  script:\n  \"\"\"\n  salmon index --transcripts $transcriptome --index salmon_index\n  \"\"\"\n}\n</code></pre> <p>It contains: </p> <ul> <li>The empty <code>input:</code> block for us to define the input data for the process. </li> <li>The empty <code>output:</code> block for us to define the output data for the process.</li> <li>The <code>script:</code> block prefilled with the command that will be executed.</li> </ul> <p>Info</p> <p>The process <code>script</code> block is executed as a Bash script by default. In Part 2 of the workshop, we will only be using Nextflow variables within the <code>script</code> block.</p> <p>Note how we have modified the original bash script in two ways.</p> <p>First, we have replaced the hard-coded path to the transcriptome file with a Nextflow variable: <code>$transcriptome</code>. This will allow us to use this process to index any transcriptome FASTA file we want without having to modify the command itself.</p> <p>Second, we removed the creation of a <code>results</code> directory and simply told Salmon to create the new index <code>salmon_index</code> within the current directory. The original script created a <code>results</code> directory as a way to organise its outputs neatly, but we don't need to worry about being so neat here; instead, as you will see below, we will use the <code>publishDir</code> to neatly organise our outputs.</p> <p>Next, we will edit the <code>input</code> and <code>output</code> blocks to match the expected data and results for this process. Looking back at our original bash script <code>00_index.sh</code>, we can see that:  </p> <ul> <li>Our input is a FASTA (<code>.fa</code>) file, now represented by the variable <code>$transcriptome</code> and provided to the <code>--transcripts</code> flag  </li> <li>The name of the index output directory, defined by using the <code>--index</code> flag, is called <code>salmon_index/</code> </li> </ul> <p>Defining inputs and outputs</p> <p>Remember, input and output definitions require a qualifier and name. For example: <pre><code>input:\n&lt;input qualifier&gt; &lt;input name&gt;\n\noutput:\n&lt;output qualifier&gt; &lt;output name&gt;\n</code></pre></p> <p>The qualifier defines the type of data, and the names are treated like variables.</p> main.nf<pre><code>process INDEX {\n  [ directives ]\n\n  input:\n  path transcriptome\n\n  output:\n  path 'salmon_index'\n\n  script:\n  \"\"\"\n  salmon index --transcripts $transcriptome --index salmon_index\n  \"\"\"\n}\n</code></pre> <p>Note that the input <code>path transcriptome</code> refers to a variable, meaning the actual file or directory provided as input can be changed depending on the data you provide it. The output <code>path 'salmon_index'</code> is fixed, meaning it will always expect an output folder called <code>salmon_index</code>, no matter what the input is.  </p> <p>This is how Nextflow can handle different inputs while always producing the same output name.  </p> <p>More information on using input and output blocks can be found in the Nextflow documentation for process inputs and outputs.  </p>"},{"location":"part2/01_salmon_idx/#212-saving-our-output-files-to-an-output-directory-with-publishdir","title":"2.1.2 Saving our output files to an output directory with <code>publishDir</code>","text":"<p>Next we will implement the Nextflow equivalent of saving the output files into a <code>results/</code> directory. Recall from Part 1  that we can use the <code>publishDir</code> directive to accomplish this. This will direct Nextflow to copy all of the process' outputs to a given directory.</p> <p>We can also specify how Nextflow will copy our results to the <code>publishDir</code> directory by setting the <code>mode</code>. We can tell Nextflow to make a complete copy of the outputs by setting the <code>mode</code> to <code>\"copy\"</code>, e.g.:</p> <pre><code>publishDir \"results\", mode: 'copy'\n</code></pre> Advanced content: <code>publishDir</code> modes <p>The <code>publishDir</code> directive has several modes that change how it behaves. By default, Nextflow will create a symbolic link in the publishing directory. This is a special type of file that points to another file, similar to a shortcut. The advantage of using a symbolic link is that it doesn't require duplicating the output files, meaning it uses less space and is quick to make. The disadvantage, however, is that the actual data is still stored in the <code>work/</code> directory. If you ever clean up the <code>work/</code> directory, you will break the symbolic links and you will lose your data.</p> <p>We will be using the <code>\"copy\"</code> mode for the remainder of this workshop, which tells Nextflow to make a complete copy of the data within the publishing directory, thereby ensuring that the final outputs of the pipeline are always available there.</p> <p>The other modes that are available are described in detail in the Nextflow documentation.</p> <p>Replace the <code>[ directives ]</code> placeholder in your <code>main.nf</code> script with the <code>publishDir</code>  directive, specifying the directory name as <code>\"results\"</code> and the mode as <code>'copy'</code>. Your <code>main.nf</code> should look like this: </p> main.nf<pre><code>process INDEX {\n  publishDir \"results\", mode: 'copy'\n\n  input:\n  path transcriptome\n\n  output:\n  path 'salmon_index'\n\n  script:\n  \"\"\"\n  salmon index --transcripts $transcriptome --index salmon_index\n  \"\"\"\n}\n</code></pre> <p>This process is now directed to copy all output files (i.e. the <code>salmon_index</code> directory) into a <code>results/</code> directory. This saves having to specify the output directory in the script definition each process, or a tedious <code>mv salmon_index/ results/</code> step. </p> <p>Nextflow also handles whether the directory already exists or if it should be created. In the <code>00_index.sh</code> script you had to manually make a  results directory with <code>mkdir -p \"results</code>.</p> <p>More information and other modes can be found on publishDir.</p>"},{"location":"part2/01_salmon_idx/#213-adding-params-and-the-workflow-scope","title":"2.1.3 Adding <code>params</code> and the workflow scope","text":"<p>Now that you have written your first Nextflow process, we need to prepare it for execution.  </p> <p>You can think of Nextflow processes as similar to a function definition in R or Python. We have defined what the process should do, but to actually run it, we need to call the process within the workflow and pass in the inputs.</p> <p>To run the process, we need to call it inside the <code>workflow{}</code> block, where we control how data flows through the pipeline. To provide the input data we need to define parameters. </p> <p>In the <code>00_index.sh</code> script, the file <code>data/ggal/transcriptome.fa</code> was passed as the input into <code>salmon index</code>.  </p> <p>We will pass in this file path with the <code>params</code> scope. Add the following to  the top of your <code>main.nf</code> script:  </p> main.nf<pre><code>// pipeline input parameters\nparams.transcriptome_file = \"$projectDir/data/ggal/transcriptome.fa\"\n</code></pre> <p>Implicit variables in Nextflow</p> <p>Nextflow provides a set of implicit variables that can be used in your workflows. These variables are predefined and can be used to access information about the workflow environment, configuration, and tasks. </p> <p>We will use <code>$projectDir</code> to indicate the directory of the <code>main.nf</code> script. This is defined by Nextflow as the directory where the <code>main.nf</code> script is located.</p> <p>The <code>params</code> and <code>process</code> names do not need to match!</p> <p>In the <code>INDEX</code> process, we defined the input as a path called <code>transcriptome</code>, whereas the parameter is called <code>transcriptome_file</code>. These do not need to be identical names as they are called in different scopes (the <code>INDEX</code> process scope, and <code>workflow</code> scope, respectively).</p> <p>Recall that parameters are inputs and options that can be customised when the workflow is  executed. They allow you to control things like file paths and options for  tools without changing the process code itself.  </p> <p>We defined a default value for <code>params.transcriptome_file</code> in the <code>main.nf</code> script.  If we need to run our pipeline with a different transcriptome  file, we can overwrite this default in our execution command with  <code>--transcriptome_file</code> double hyphen flag.</p> <p>Next, add the workflow scope at the bottom of your <code>main.nf</code> after the process:  </p> main.nf<pre><code>// Define the workflow\nworkflow {\n\n    // Run the index step with the transcriptome parameter\n    INDEX(params.transcriptome_file)\n}\n</code></pre> <p>This will tell Nextflow to run the <code>INDEX</code> process with <code>params.transcriptome_file</code> as input.</p> <p>Tip: Your own comments</p> <p>As a developer you can to choose how and where to comment your code! Feel free to modify or add to the provided comments to record useful information about the code you are writing.</p> <p>Exercise</p> <p>Now that we have a complete process and workflow, we should be able to run it!</p> <p>In your VSCode terminal, ensure you are inside the <code>part2/</code> directory, then run the following:</p> <pre><code>nextflow run main.nf\n</code></pre> <p>What happened? Did the pipeline run successfully? If not, why not?</p> Solution <p>Something went wrong! The pipeline didn't successfully run!</p> <p>You should have received an error message similar to the following:</p> Output<pre><code> N E X T F L O W   ~  version 24.10.2\n\nLaunching `main.nf` [peaceful_neumann] DSL2 - revision: caa2043bdf\n\nexecutor &gt;  local (1)\nexecutor &gt;  local (1)\n[eb/ec6173] process &gt; INDEX [100%] 1 of 1, failed: 1 \u2718\nERROR ~ Error executing process &gt; 'INDEX'\n\nCaused by:\nProcess `INDEX` terminated with an error exit status (127)\n\n\nCommand executed:\n\nsalmon index --transcripts transcriptome.fa --index salmon_index\n\nCommand exit status:\n127\n\nCommand output:\n(empty)\n\nCommand error:\n.command.sh: line 2: salmon: command not found\n\nWork dir:\n/home/training/_part2/work/eb/ec617307600cd47fc5b65d1c60269e\n\nTip: you can try to figure out what's wrong by changing to the process work dir and showing the script file named `.command.sh`\n\n-- Check '.nextflow.log' file for details\n</code></pre> <p>Nextflow's error message tells us exactly why this failed. The highlighted line above tells us that the <code>salmon</code> command couldn't be found.</p> <p>Why did this happen? Because we don't have <code>salmon</code> installed!</p> <p>How do we fix this? We'll explore that in the very next section...</p>"},{"location":"part2/01_salmon_idx/#214-using-containers-for-reproducible-pipelines","title":"2.1.4 Using containers for reproducible pipelines","text":"<p>One of the primary goals of workflow managers like Nextflow is to improve the portability of a pipeline. Ideally, we could take our pipeline to any computer and it would run with minimal setup - in short, \"write once, run anywere\".</p> <p>However, all but the most trivial workflows will require specialised software that can't be assumed to be installed on any given computer. To ensure that the pipeline requires \"minimal setup\", we can't be asking the end users to install every piece of software the pipeline requires. So how can we ensure that the end user can run our pipeline without needing to install all of its dependencies?</p> <p>The answer is containers.</p> <p>Containers package all the software and dependencies needed for each tool into a self-contained environment. This means you don\u2019t have to manually install anything on your system, and your workflow will work consistently across different systems \u2014 whether you're running it on your local machine, a cluster, or in the cloud. Containers make it easier to share your workflow with others and ensure it runs the same way every time, no matter where it's executed.</p> <p>Nextflow recommends as a best practice to use containers to ensure the reproducibility and portability of your workflows.</p> <p>Nextflow supports multiple container runtimes. In this workshop, we'll be demonstrating the value containers can bring to your workflow by using Singularity.</p> Tip: different tools for different purposes <p>In this workshop, we're using Singularity to run containers. You may have heard of another container technology before: Docker. Both Singularity and Docker work in similar ways to encapuslate tools within an environment to ensure reproducibility. However, Docker has certain administrative access requirements that make it unsuitable for some systems like HPCs. For this reason, we will be working with Singularity.</p> <p>You don't have to write your own containers to run in your workflow. There are many container repositories out there. We highly recommend using  Biocontainers wherever possible. Biocontainers are pre-built and tested containers specifically for bioinformatics tools. They have a huge library and great community support. </p> <p>You can find Biocontainers at the following repositories:  </p> <ul> <li>Biocontainers registry</li> <li>Quay.io</li> <li>DockerHub</li> <li>Seqera containers</li> </ul> <p>Another helpful fact is that Docker containers are often able to be converted to Singularity's format, meaning if a tool is only available as a Docker image, it is highly likely that it can still be used with Singularity.</p> <p>In Nextflow, we can specify that a process should be run within a specified container using the container directive.  </p> <p>Add the following container directive to the <code>INDEX</code> process, above <code>publishDir</code>:  </p> main.nf<pre><code>process INDEX {\n    container \"quay.io/biocontainers/salmon:1.10.1--h7e5ed60_0\"\n    publishDir \"results\", mode: 'copy'\n\n    input:\n    path transcriptome\n\n    output:\n    path 'salmon_index'\n\n    script:\n    \"\"\"\n    salmon index --transcripts $transcriptome --index salmon_index\n    \"\"\"\n}\n</code></pre> <p>Tip</p> <p>Usually, containers need to be downloaded using a command such as <code>singularity pull [image]</code>. All containers have been previously downloaded for the workshop beforehand.</p> Tip: use one container per process <p>Using single containers for each process in your workflow is considered best practices for the following reasons:</p> <ul> <li>Flexibility: different processes require different tools (or versions). By using separate containers, you can easily tailor the container to the needs of each process without conflicts.</li> <li>Build and run efficiency: Smaller, process-specific containers are faster to load and run compared to one large container that has unnecessary tools or dependencies for every process.</li> <li>Easier Maintenance: it\u2019s easier to update or modify one container for a specific process than to manage a large, complex container with many tools.</li> <li>Reproducibility: reduces the risk of issues caused by software conflicts.</li> </ul> <p>Now our process has a container associated with it, but before we can run the workflow, we need to tell Nextflow how to run the container. Specifically, we need to specify that we want to run containers using Singularity. Note that this also requires Singularity to be installed on your system in order for this to work. Singularity has been  pre-installed on your Virtual Machine. See the Nextflow documentation for further details on running workflows with Singularity.</p> <p>We can configure Nextflow to run containers with Singularity by using the  <code>nextflow.config</code> file.</p> <p>Create a <code>nextflow.config</code> file in the same directory as <code>main.nf</code>.  </p> <p>Note</p> <p>You can create the file via the VSCode Explorer (left sidebar) or in the terminal with a text editor.</p> <p>If you are using the Explorer, right click on <code>part2</code> in the sidebar and select \"New file\".</p> <p>Add the following lines to your config file:</p> nextflow.config<pre><code>singularity {\n    enabled = true\n    cacheDir = \"$HOME/singularity_image\"\n}\n</code></pre> <p>The syntax <code>singularity { }</code> defines the configuration for using Singularity; everything between the curly braces here will tell Nextflow how to use Singularity to run your workflow.</p> <p>The first line, <code>enabled = true</code> simply tells Nextflow to use Singularity. The second line, <code>cacheDir = $HOME/singularity_image</code> tells Nextflow to store images in a folder within your home directory called <code>singularity_image</code>. This means that Nextflow only has to pull a given image from the internet once; every other time it requires that image, it can quickly load it from this cache directory.</p> <p>You have now configured Nextflow to run your process within a Singularity container! In this case, the <code>INDEX</code> process will use the <code>quay.io/biocontainers/salmon:1.10.1--h7e5ed60_0</code> container. As we add more processes, wherever we define the <code>container</code> directive, Nextflow will use that container to run that process.</p> <p>Tip</p> <p>Remember to save your files after editing them!</p> <p>We now have a complete process and workflow, along with directives and configuration for using containers. We are ready to try running our workflow again!</p>"},{"location":"part2/01_salmon_idx/#215-running-the-workflow","title":"2.1.5 Running the workflow","text":"<p>In the terminal, run the command:  </p> <pre><code>nextflow run main.nf\n</code></pre> <p>Your output should look something like:  </p> Output<pre><code>N E X T F L O W   ~  version 24.10.2\n\nLaunching `main.nf` [chaotic_jones] DSL2 - revision: 6597720332\n\nexecutor &gt;  local (1)\n[de/fef8c4] INDEX | 1 of 1 \u2714\n</code></pre> <p>Recall that the specifics of the output are randomly generated (i.e. <code>[chaotic_jones]</code> and <code>[de/fef8c4]</code> in this example).</p> <p>In this example, the output files for the <code>INDEX</code> process is output in <code>work/de/fef8c4...</code>.</p> <p>You have successfully run your first workflow!  </p>"},{"location":"part2/01_salmon_idx/#216-inspecting-the-outputs","title":"2.1.6 Inspecting the outputs","text":"<p>You will notice that we have two new folders in our working directory: <code>results/</code> and <code>work/</code>.</p> <p>First, let's look at the <code>results/</code> directory. It should contain a single subdirectory: <code>salmon_index/</code>. Inside are all the files that make up the <code>salmon</code> reference index used for quantifying reads.</p> <pre><code>ls results\n</code></pre> Output<pre><code>salmon_index\n</code></pre> <pre><code>ls results/salmon_index\n</code></pre> Output<pre><code>complete_ref_lens.bin   mphf.bin             ref_indexing.log\nctable.bin              pos.bin              reflengths.bin\nctg_offsets.bin         pre_indexing.log     refseq.bin\nduplicate_clusters.tsv  rank.bin             seq.bin\ninfo.json               refAccumLengths.bin  versionInfo.json\n</code></pre> <p>This is the exact same directory structure that our original <code>00_index.sh</code> script was creating when running <code>mkdir \"results\"</code> and passing the <code>--index results/salmon_index</code> parameter to <code>salmon</code>. But now, Nextflow is handling this for us thanks to the <code>publishDir</code> directive we gave to the <code>INDEX</code> process. That directive told Nextflow to create the <code>results/</code> directory (if it didn't already exist) and copy the <code>salmon_index</code> output into it.</p> <p>The other directory that has been created is <code>work/</code>. This is where Nextflow runs all of our processes and stores all of their associated files. Understanding how the <code>work/</code> directory is organised can be very useful for debugging and diagnosing problems with your processes.</p> <p>You may recall from Part 1.3, every time a process is run, it is given a randomly-generated ID, such as <code>ec/9ed7c7d13ca353bbd8e99835de8c47</code>. This helps Nextflow uniquely identify each instance of every process. You will see a truncated form of this ID printed to the terminal when running Nextflow:</p> Nextflow output<pre><code>    N E X T F L O W   ~  version 24.10.2\n\nLaunching `main.nf` [sleepy_volhard] DSL2 - revision: c2ada21e4e\n\nexecutor &gt;  local (1)\n[ec/9ed7c7] process &gt; INDEX [100%] 1 of 1 \u2714\n</code></pre> <p>Nextflow creates a directory inside <code>work/</code> with this same path for each instance of every process. Inside will be the output of that process as well as copies of its inputs:</p> <pre><code>ls work/ec/9ed7c7d13ca353bbd8e99835de8c47  # Your work directory will have a different name\n</code></pre> Output<pre><code>salmon_index  transcriptome.fa\n</code></pre> <p>There will also be several hidden files that Nextflow uses to run and monitor the process:</p> <pre><code>ls -A work/ec/9ed7c7d13ca353bbd8e99835de8c47  # Your work directory will have a different name\n</code></pre> Output<pre><code>.command.begin  .command.out  .exitcode\n.command.err    .command.run  salmon_index\n.command.log    .command.sh   transcriptome.fa\n</code></pre> <p>Of particular interest to us right now is the <code>.command.sh</code> file, which contains our process script.</p> <p>Exercises</p> <p><ol> <li>Navigate to the <code>work/</code> directory and open the <code>.command.sh</code> file.</li> <li>Compare the <code>.command.sh</code> file with our <code>INDEX</code> process. How do they differ? How are they similar?</li> </ol></p> Solution <p>The command in <code>.command.sh</code> should look very similar to the script block in the <code>INDEX</code> process, except now the Nextflow variable <code>$transcriptome</code> has been replaced by a relative path to the <code>transcriptome.fa</code> file present in the working directory:</p> main.nf<pre><code>process INDEX {\n    ...\n\n    script:\n    \"\"\"\n    salmon index -t $transcriptome -i salmon_index\n    \"\"\"\n}\n</code></pre> .command.sh<pre><code>#!/bin/bash -ue\nsalmon index -t transcriptome.fa -i salmon_index\n</code></pre> <p><ol> <li>How does <code>.command.sh</code> compare to the original <code>00_index.sh</code> script?</li> </ol></p> Solution <p>The <code>.command.sh</code> script contains a hard-coded path to the copy of the <code>transcriptome.fa</code> file within the task directory, which closely resembles our hard-coded <code>salmon</code> command in our original <code>00_index.sh</code> script:</p> 00_index.sh<pre><code>salmon index \\\n    --transcripts data/ggal/transcriptome.fa \\\n    --index results/salmon_index\n</code></pre> .command.sh<pre><code>#!/bin/bash -ue\nsalmon index -t transcriptome.fa -i salmon_index\n</code></pre> <p>In a sense, Nextflow is doing the same thing we would have had to do with our original set of bash scripts: make a copy, modify the hard-coded values, and run them. Except now, it is all being automated and handled for us by Nextflow!</p> Advanced content: the <code>work/</code> directory hidden files <p>The other hidden files inside the work directory have the following roles:</p> <ul> <li><code>.command.run</code>: A \"wrapper script\" that contains all the behind-the-scenes logic to run and monitor <code>.command.sh</code>.</li> <li><code>.command.out</code>: The standard output log from <code>.command.sh</code>.</li> <li><code>.command.err</code>: The standard error log from <code>.command.sh</code>.</li> <li><code>.command.log</code>: The output log generated by the <code>.command.run</code> wrapper script.</li> <li><code>.command.begin</code>: A file created as soon as the job is launched.</li> <li><code>.exitcode</code>: A file created at the end of the job containing the task exit code (0 if successful)</li> </ul> <p>Summary</p> <p>In this lesson you have learned:  </p> <ol> <li>How to implement a simple process with one input file  </li> <li>How to define parameters in your workflow scripts and the command line</li> <li>How to use configure a process to run using a container   </li> <li>How to output files in a dedicated <code>publishDir</code> </li> </ol>"},{"location":"part2/02_fastqc/","title":"2.2 Samplesheets, operators, and groovy","text":"<p>Learning objectives</p> <ol> <li>Construct a Nextflow process that accepts a tuple input from a channel.</li> <li>Describe the benefits of using a samplesheet to manage and read in workflow inputs.</li> <li>Build a custom input channel using Groovy expressions and Nextflow operators. </li> </ol> <p>In this lesson we will transform the next bash script, <code>01_fastqc.sh</code> into a process called <code>FASTQC</code>. This step focuses on the next phase of RNAseq data processing: assessing the quality of some our raw sequencing reads. </p> <p>To do this, we will need to run FastQC  over pairs of FASTQ files. </p> <p></p> <p>Our goal in porting these bash scripts to Nextflow is to build a workflow that can scale to run on multiple samples with minimal intervention. To do this, we will use a samplesheet, allowing us to provide multiple samples and their corresponding FASTQ files to our Nextflow workflow. </p> <p>Building channels in Nextflow can be tricky. Depending on what data you need  to capture and how you want to organise it you will likely need to use  operators to manipulate your channel. We saw some simple operators back in Part 1. However, sometimes operators alone won't be enough, and you'll need to also use Groovy (Nextflow's underlying programming language) to capture pertinent information.  </p> <p>Since this is an advanced task, we will provide you with all the code you need. Although Nextflow does not yet offer a built-in operator for reading samplesheets, their use is widespread in bioinformatics workflows. So, we will be building a simple samplesheet reader from a couple of operators and some simple Groovy code.</p>"},{"location":"part2/02_fastqc/#221-inspecting-our-fastqc-script","title":"2.2.1 Inspecting our FastQC script","text":"<p>Open the bash script <code>01_fastqc.sh</code>:  </p> 01_fastqc.sh<pre><code>SAMPLE_ID=gut\nREADS_1=\"data/ggal/${SAMPLE_ID}_1.fq\"\nREADS_2=\"data/ggal/${SAMPLE_ID}_2.fq\"\n\nmkdir -p \"results/fastqc_${SAMPLE_ID}_logs\"\nfastqc \\\n    --outdir \"results/fastqc_${SAMPLE_ID}_logs\" \\\n    --format fastq ${READS_1} ${READS_2}\n</code></pre> <p>There's a lot going on in this script, let's break it down.</p> <p><code>SAMPLE_ID=gut</code> assigns \"gut\" to the bash variable<code>SAMPLE_ID</code>. This is used to:  </p> <ul> <li>Avoid hardcoding the sample name multiple times in the script  </li> <li>Ensure that file pairs of the same sample are processed together (e.g. <code>gut_1.fq</code> and <code>gut_2.fq</code>)  </li> <li>Ensure that this script can be run on different sample pairs  </li> </ul> <p><code>READS_1</code> and <code>READS_2</code> specify the paths to the paired gut <code>.fq</code> files.  </p> <p>Similar to the bash script in the previous step (<code>00_index.sh</code>), <code>mkdir -p</code> creates an output folder so that the <code>fastqc</code> outputs can be saved here.  </p> <p>In the <code>fastqc</code> command,</p> <ul> <li><code>--outdir</code> specifies the name of the output directory</li> <li><code>--format</code> is a required flag to indicate what format the the reads are in</li> <li><code>${READS_1}</code> and <code>${READS_2}</code> propagate the paths of the <code>.fq</code> files  </li> </ul>"},{"location":"part2/02_fastqc/#222-building-the-fastqc-process","title":"2.2.2 Building the <code>FASTQC</code> process","text":""},{"location":"part2/02_fastqc/#defining-the-process-directives","title":"Defining the process directives","text":"<p>Start by adding the following <code>process</code> scaffold and script definition to your <code>main.nf</code> under the INDEX process code but before the <code>workflow{}</code> block:  </p> main.nf<pre><code>process FASTQC {\n  container \"quay.io/biocontainers/fastqc:0.12.1--hdfd78af_0\"\n  publishDir \"results\", mode: 'copy'\n\n  input:\n    &lt; process inputs &gt;\n\n  output:\n    &lt; process outputs &gt;\n\n  script:\n  \"\"\"\n  mkdir -p \"fastqc_${sample_id}_logs\"\n  fastqc --outdir \"fastqc_${sample_id}_logs\" --format fastq $reads_1 $reads_2\n  \"\"\"\n}\n</code></pre> <p>It contains: </p> <ul> <li>Prefilled process directives <code>container</code> and <code>publishDir</code>.</li> <li>The empty <code>input:</code> block for us to define the input data for the process. </li> <li>The empty <code>output:</code> block for us to define the output data for the process.</li> <li>The <code>script:</code> block prefilled with the command that will be executed.</li> </ul> <p>Note that for the script block we have removed the initial three lines that contained the bash variable definitions. Instead, we will be using Nextflow variables that are defined within the process' <code>input</code> block.</p> <p>However, note that the <code>mkdir</code> and <code>fastqc</code> commands that remain look very similar to their original forms, but are now using those Nextflow variables instead of the original bash variables.</p> <p>Dynamic naming</p> <p>Recall that curly brackets are used to pass variables as part of a file name.</p> Advanced content: Nextflow vs Bash variables <p>If you are familiar with Bash programming, you may notice that the way we use Nextflow variables looks exactly like how we use Bash variables - by using the <code>$</code> symbol followed by the variable name, possibly within curly brackets:</p> <pre><code>script:\n\"\"\"\nmkdir -p \"fastqc_${sample_id}_logs\"\n\"\"\"\n</code></pre> <p>This actually means that Bash variables can't be used in the same way as they normally are. Instead, if you ever need to use a Bash variable within a Nextflow process, you will first need to escape the <code>$</code> symbol with a backslash (<code>\\</code>). This tells Nextflow to ignore the <code>$</code> and not interpret it as a Nextflow variable:</p> <pre><code>script:\n\"\"\"\nSOMEBASHVAR=\"hello\"\necho \\${SOMEBASHVAR}\n\"\"\"\n</code></pre> <p>For the purposes of this workshop, we won't be using Bash variables, so you don't need to worry about this quirk for now.</p>"},{"location":"part2/02_fastqc/#defining-the-process-output","title":"Defining the process <code>output</code>","text":"<p>Unlike <code>salmon</code> from the previous process, <code>fastqc</code> requires that the output directory be created before running the command, hence the requirement to run <code>mkdir -p \"fastqc_${sample_id}_logs\"</code> within the <code>script</code> block. This is a common  inconsistency between different bioinformatics tools, so it is good to be aware of it.</p> <p>Looking at the FastQC command we can see this directory will be our output.  </p> <p>Exercise</p> <p>Replace <code>&lt; process outputs &gt;</code> with the appropriate output definition for the <code>FASTQC</code> process.  </p> Solution main.nf<pre><code>output:\npath \"fastqc_${sample_id}_logs\"\n</code></pre> <p>We've used the <code>path</code> qualifier because our output is a directory. Output from the bash script is defined by the fastqc <code>--outdir</code> flag. Because our output contains constant text (i.e. <code>\"fastqc_\"</code> and <code>\"_logs\"</code>), we put the whole output string in double quotes and use the dollar sign and curly braces to access the <code>sample_id</code> variable.</p>"},{"location":"part2/02_fastqc/#defining-the-process-input","title":"Defining the process <code>input</code>","text":"<p>Now we need to define the <code>input</code> block for this process. In this process,  we're going to use a combination of Nextflow operators and Groovy to do this. </p> <p>There are three inputs for this process definition that can be taken from the script definition you just added:</p> <ol> <li><code>$sample_id</code></li> <li><code>$reads_1</code></li> <li><code>$reads_2</code></li> </ol> <p>In order to ensure we process the sample ID along with its two related FASTQ files together, we will introduce a new input qualifier: the <code>tuple</code>.</p> <p>A tuple is simply an ordered collection of objects. When you use a tuple as input to a Nextflow process, it ensures that the objects inside are grouped and processed together as a single unit. This is a requirement when working with multiple pieces of data that are specific to a given sample.</p> <p>Importance of proper data grouping when using Nextflow</p> <p>Nextflow uses channels to run processes in parallel and if you aren't careful about how you handle multiple pieces of related data that need to be tied together (e.g. sample IDs and FASTQ paths), you may mix datasets up.</p> <p>We can use the input qualifier <code>tuple</code> to group multiple values into a single input definition.</p> <p>In our case, we have three related pieces of data: a <code>$sample_id</code> and its two FASTQ read files <code>$reads_1</code>  and <code>$reads_2</code>. We will define a tuple input to ensure that these inputs stay linked and are processed together, preventing our sample IDs and FASTQ files from getting mixed up between samples.</p> <p></p> <p>In the <code>FASTQC</code> process, replace <code>&lt; process inputs &gt;</code> with the input tuple definition:  </p> <pre><code>tuple val(sample_id), path(reads_1), path(reads_2)\n</code></pre> <p>Our tuple contains three elements:</p> <ul> <li><code>val(sample_id)</code> represents the value that refers to the sample name.</li> <li><code>path(reads_1)</code> represents the path to the first read file of paired-end sequencing data.</li> <li><code>path(reads_2)</code> represents the path to the second read file of paired-end sequencing data.</li> </ul> <p>Note how each item within the tuple has its own qualifier. Also note how we must now wrap each item's name within parentheses, and separte them by commas.</p> <p>Our <code>FASTQC</code> process should now look like this:</p> main.nf<pre><code>process FASTQC {\n  container \"quay.io/biocontainers/fastqc:0.12.1--hdfd78af_0\"\n  publishDir \"results\", mode: 'copy'\n\n  input:\n  tuple val(sample_id), path(reads_1), path(reads_2)\n\n  output:\n  path \"fastqc_${sample_id}_logs\"\n\n  script:\n  \"\"\"\n  mkdir -p \"fastqc_${sample_id}_logs\"\n  fastqc --outdir \"fastqc_${sample_id}_logs\" --format fastq $reads_1 $reads_2\n  \"\"\"\n}\n</code></pre>"},{"location":"part2/02_fastqc/#223-reading-files-with-a-samplesheet","title":"2.2.3 Reading files with a samplesheet","text":"<p>Up until this point in the workshop, we have been using a lot of hard-coded values.</p> <p>In practice, hard-coded values, particularly for file names and sample IDs, should almost never be used. Instead, we need a flexible way of providing variable values to our pipeline. For that purpose, we can use a samplesheet.</p> <p>A samplesheet is a delimited text file where each row contains information or metadata that needs to be processed together.</p> <p></p> Tip: using samplesheets in scalable bioinformatics workflows <p>Working with samplesheets is particularly useful when you have a combination of files and metadata that need to be assigned to a sample in a flexible manner. Typically, samplesheets are written in comma-separated (<code>.csv</code>) or tab-separated (<code>.tsv</code>) formats. </p> <p>We recommend using comma-separated files as they are less error prone and easier to read and write.</p> <p>Let's inspect <code>data/samplesheet.csv</code> with VSCode.</p> Output<pre><code>sample,fastq_1,fastq_2\ngut,data/ggal/gut_1.fq,data/ggal/gut_2.fq\n</code></pre> <p>Think of this file like a table; each line is a row and within each row are multiple 'columns' delimited by comma symbols (<code>,</code>). This samplesheet has two rows. The first is a header row; the values here will be used as names for each of the columns. The second row is a single sample, <code>gut</code>. At this stage, we are developing and testing the pipeline, so, we're only working with one sample. The samplesheet has three columns:  </p> <ul> <li><code>sample</code>: indicates the sample name/prefix (in this case: <code>gut</code>)</li> <li><code>fastq_1</code>, <code>fastq_2</code>: contains the relative paths to the paired read FASTQ files (in this case: <code>data/ggal/gut_1.fq</code> and <code>data/ggal/gut_2.fq</code>)</li> </ul> <p></p> <p>The goal in this step is to read the contents of the samplesheet, and transform it so it fits the input definition of <code>FASTQC</code> we just defined:</p> <pre><code>tuple val(sample_id), path(reads_1), path(reads_2)\n</code></pre> <p>Before that, we need to add an input parameter that points to the samplesheet, called <code>reads</code>.  </p> <p>Exercise</p> <p>In your <code>main.nf</code> add a new input parameter called <code>reads</code> and assign it a default path to the samplesheet, using the <code>$projectDir</code> variable as before.</p> Solution main.nf<pre><code>// pipeline input parameters\nparams.transcriptome_file = \"$projectDir/data/ggal/transcriptome.fa\"\nparams.reads = \"$projectDir/data/samplesheet.csv\"\n</code></pre> <p>In the next few steps, we will add a mix of Nextflow operators and Groovy syntax to read in and parse the samplesheet so it is in the correct format for the process we just added.  </p> <p>Using samplesheets with Nextflow can be tricky business</p> <p>There are currently no Nextflow operators specifically designed to handle samplesheets. As such, we Nextflow workflow developers have to write custom parsing logic to read and split the data. This adds complexity to our workflow development, especially when trying to handle tasks like parallel processing of samples or filtering data by sample type.</p> <p>Add the following to your workflow scope below where <code>INDEX</code> is called:</p> main.nf<pre><code>// Define the workflow  \nworkflow {\n\n    // Run the index step with the transcriptome parameter\n    INDEX(params.transcriptome_file)\n\n    // Define the fastqc input channel\n    Channel.fromPath(params.reads)\n        .splitCsv(header: true)\n        .map { row -&gt; [row.sample, file(row.fastq_1), file(row.fastq_2)] }\n        .view()\n\n}\n</code></pre> <p>This looks a bit complicated, but the essential idea is that we read in our samplesheet file with the <code>.fromPath</code> channel factory and the <code>.splitCSV</code> operator and transform the data into our final <code>tuple</code> format with the <code>.map</code> operator. The key takeaway here is to understand that using samplesheets is best practice for reading grouped files and metadata into Nextflow, and that both Nextflow operators and Groovy code need to be chained together to get these in the correct format.</p> Advanced content: A deeper look at the samplesheet logic <p>Our samplesheet input channel has used a few common Nextflow operators chained together:</p> <ul> <li><code>.fromPath</code> is a channel factory that creates a channel from one or more files matching a given path or pattern. In this case, this is our samplesheet <code>.csv</code> file contained in the <code>params.reads</code> parameter, provided with the <code>--reads</code> command line flag.</li> <li><code>.splitCsv</code> splits the input file into rows, treating it as a CSV (Comma-Separated Values) file. The <code>header: true</code> option means that the first row of the CSV contains column headers, which will be used to access the values by name.</li> <li><code>.map { row -&gt; [row.sample, file(row.fastq_1), file(row.fastq_2)] }</code> uses the <code>.map</code> operator and some Groovy syntax to transform each row of the CSV file into a tuple, extracting the <code>sample</code> value, as well as the <code>fastq_1</code> and <code>fastq_2</code> file paths from the row.</li> <li><code>.view()</code> is a debugging step that outputs the transformed data to the console so we can see how the channel is structured. It's a great tool to use when building your channels.</li> </ul> Tip: using the <code>view()</code> operator for testing <p>The <code>view()</code> operator is a useful tool for debugging Nextflow workflows. It allows you to inspect the data structure of a channel at any point in the workflow, helping you to understand how the data is being processed and transformed.</p> <p>Run the workflow with the <code>-resume</code> flag:</p> <pre><code>nextflow run main.nf -resume\n</code></pre> <p>Your output should look something like:  </p> Output<pre><code>Launching `main.nf` [crazy_einstein] DSL2 - revision: 0ae3776a5e\n\n[de/fef8c4] INDEX [100%] 1 of 1, cached: 1 \u2714\n[gut, /home/setup2/hello-nextflow/part2/data/ggal/gut_1.fq, /home/setup2/hello-nextflow/part2/data/ggal/gut_2.fq]\n</code></pre> Tip: using the <code>-resume</code> flag <p>The <code>-resume</code> flag is used to resume a Nextflow workflow from where it left off. If a workflow fails or is interrupted, this flag allows you to skip tasks that were successfully completed, saving time and computational resources. It is also useful when you are developing a workflow and want to test changes without running the entire workflow from the start. </p> <p>The chain of commands produces a tuple with three elements that correspond to the row in the samplesheet. It now fits the requirements of the input definition of <code>tuple val(sample_id), path(reads_1), path(reads_2)</code>: </p> <pre><code>[gut, /home/setup2/hello-nextflow/part2/data/ggal/gut_1.fq, /home/setup2/hello-nextflow/part2/data/ggal/gut_2.fq]\n</code></pre> <p>How's it going?</p> <p>Once you have run the workflow, select the  \"Yes\" react on Zoom.</p> <p>Next, we need to assign the channel we create to a variable so it can be passed to the <code>FASTQC</code> process. Assign to a variable called <code>reads_in</code>, and remove the <code>.view()</code> operator as we now know what the output looks like.</p> main.nf<pre><code>// Define the workflow  \nworkflow {\n\n    // Run the index step with the transcriptome parameter\n    INDEX(params.transcriptome_file)\n\n    // Define the fastqc input channel\n    reads_in = Channel.fromPath(params.reads)\n        .splitCsv(header: true)\n        .map { row -&gt; [row.sample, file(row.fastq_1), file(row.fastq_2)] }\n\n}\n</code></pre> <p>Now that we have an input channel ready that provides the correct format, we can call the <code>FASTQC</code> process.  </p> <p>Exercise</p> <p>In the <code>workflow</code> scope after where <code>reads_in</code> was defined, call the <code>FASTQC</code> process with <code>reads_in</code> as the input.</p> Solution main.nf<pre><code>// Define the workflow  \nworkflow {\n\n    // Run the index step with the transcriptome parameter\n    INDEX(params.transcriptome_file)\n\n    // Define the fastqc input channel\n    reads_in = Channel.fromPath(params.reads)\n        .splitCsv(header: true)\n        .map { row -&gt; [row.sample, file(row.fastq_1), file(row.fastq_2)] }\n\n    // Run the fastqc step with the reads_in channel\n    FASTQC(reads_in) \n    \n}\n</code></pre> <p>Run the workflow:  </p> <pre><code>nextflow run main.nf -resume\n</code></pre> <p>Your output should look something like:  </p> <pre><code>Launching `main.nf` [tiny_aryabhata] DSL2 - revision: 9a45f4957b\n\nexecutor &gt;  local (1)\n[de/fef8c4] INDEX      [100%] 1 of 1, cached: 1 \u2714\n[bb/32a3aa] FASTQC (1) [100%] 1 of 1 \u2714\n</code></pre> <p>If you inspect <code>results/fastqc_gut_logs</code> there is an <code>.html</code> and <code>.zip</code> file for each of the <code>.fastq</code> files.  </p> Advanced exercise: Inspecting our samplesheet reader <p>This advanced exercise walks through inspecing the output of the intermediate operators in the <code>reads_in</code> channel:  </p> <ul> <li><code>Channel.fromPath</code></li> <li><code>.splitCsv</code></li> </ul> <p>The current workflow block should look like:</p> main.nf<pre><code>// Define the workflow  \nworkflow {\n\n    // Run the index step with the transcriptome parameter\n    INDEX(params.transcriptome_file)\n\n    // Define the fastqc input channel\n    reads_in = Channel.fromPath(params.reads)\n        .splitCsv(header: true)\n        .map { row -&gt; [row.sample, file(row.fastq_1), file(row.fastq_2)] }\n\n    // Run the fastqc step with the reads_in channel\n    FASTQC(reads_in)\n}\n</code></pre> <p><code>Channel.fromPath</code> </p> <ol> <li>In the workflow scope, comment out the lines for <code>.splitCsv</code>, <code>.map</code>, and <code>FASTQC()</code></li> <li>Add <code>.view()</code> on the line after <code>Channel.fromPath</code> and before the commented <code>.splitCsv</code></li> <li>Run the workflow with <code>-resume</code> </li> </ol> Solution main.nf<pre><code>// Define the workflow  \nworkflow {\n\n    // Run the index step with the transcriptome parameter\n    INDEX(params.transcriptome_file)\n\n    // Define the fastqc input channel\n    reads_in = Channel.fromPath(params.reads)\n        .view()\n        //.splitCsv(header: true)\n        //.map { row -&gt; [row.sample, file(row.fastq_1), file(row.fastq_2)] }\n\n    // Run the fastqc step with the reads_in channel\n    //FASTQC(reads_in)\n\n}\n</code></pre> <p>The output of the <code>Channel.fromPath(params.reads)</code> step produces a path to the samplesheet:  </p> Output<pre><code>Launching `main.nf` [hungry_lalande] DSL2 - revision: 587b5b70d1\n\n[de/fef8c4] INDEX [100%] 1 of 1, cached: 1 \u2714\n/home/user1/part2/data/samplesheet.csv\n</code></pre> <p><code>.splitCsv</code> </p> <ol> <li>In the workflow scope, uncomment the line for <code>.splitCsv</code></li> <li>Move <code>.view()</code> to the line after <code>.splitCsv</code> (before the commented <code>.map</code> line)</li> <li>Run the workflow with <code>-resume</code> </li> </ol> Solution main.nf<pre><code>// Define the workflow  \nworkflow {\n\n    // Run the index step with the transcriptome parameter\n    INDEX(params.transcriptome_file)\n\n    // Define the fastqc input channel\n    reads_in = Channel.fromPath(params.reads)\n        .splitCsv(header: true)\n        .view()\n        //.map { row -&gt; [row.sample, file(row.fastq_1), file(row.fastq_2)] }\n\n    // Run the fastqc step with the reads_in channel\n    //FASTQC(reads_in)\n\n}\n</code></pre> <p><code>.splitCsv</code> takes the path from the <code>.fromPath</code> operator and reads the file. It outputs a queue channel with one element for each line of the CSV file. Each element of this channel is similar to a tuple, except each value is associated with its corresponding column name from the header row of the CSV file:</p> Output<pre><code>Launching `main.nf` [tiny_yonath] DSL2 - revision: 22c2c9d28f\n[de/fef8c4] INDEX | 1 of 1, cached: 1 \u2714\n[sample:gut, fastq_1:data/ggal/gut_1.fq, fastq_2:data/ggal/gut_2.fq]\n</code></pre> <p>This is called a key-value pair. Each of the values can be accessed by its key value (the column name). Our <code>.map</code> operator does exactly this (e.g. <code>row.sample</code> and <code>row.fastq_1</code>) and formats it into the final tuple that is stored as <code>reads_in</code> and passed to <code>FASTQC</code>.</p> <p>Before proceeding, ensure to uncomment the <code>.map</code> and <code>FASTQC</code> lines, and remove <code>.view()</code>.</p> <p>Summary</p> <p>In this lesson you have learned:</p> <ol> <li>How to implement a process with a tuple input</li> <li>How to construct an input channel using operators and Groovy</li> <li>How to use the <code>.view()</code> operator to inspect the structure of a channel</li> <li>How to use the <code>-resume</code> flag to skip sucessful tasks</li> <li>How to use a samplesheet to read in grouped samples and metadata</li> </ol>"},{"location":"part2/03_quant/","title":"2.3 Multiple inputs into a single process","text":"<p>Learning objectives</p> <ol> <li>Configure a Nextflow process to accept input from multiple channels.  </li> <li>Understand the importance of creating channels from process outputs to enable modular, reusable workflows.</li> <li>Link multiple Nextflow processes using channels to form a chained workflow. </li> </ol> <p>In this lesson we will transform the bash script <code>02_quant.sh</code> into a process called <code>QUANTIFICATION</code>. This step focuses on the next phase of RNAseq data processing: quantifying the expression of transcripts relative to the reference transcriptome. </p> <p>To do this, we will need to run Salmon's quant mode over the paired-end reads and the transcriptome index.  </p> <p>Open the bash script <code>02_quant.sh</code>.  </p> 02_quant.sh<pre><code>SAMPLE_ID=gut\nREADS_1=\"data/ggal/${SAMPLE_ID}_1.fq\"\nREADS_2=\"data/ggal/${SAMPLE_ID}_2.fq\"\n\nsalmon quant \\\n    --libType=U \\\n    -i results/salmon_index \\\n    -1 ${READS_1} \\\n    -2 ${READS_2} \\\n    -o results/${SAMPLE_ID}\n</code></pre> <p>Same as the previous lesson, this script contains the <code>${SAMPLE_ID}</code> variable definitions which is used to connect sample names to FASTQ file paths. Within the <code>salmon quant</code> execution command: </p> <ul> <li><code>--libType=U</code> is a required argument for Salmon. </li> <li><code>-i results/salmon_index</code> is the directory output by the <code>INDEX</code> process. </li> <li><code>-1</code> and <code>-2</code> are flags for the respective paired reads (<code>.fq</code>). </li> <li><code>-o</code> outputs files into a directory called <code>results/gut</code></li> </ul>"},{"location":"part2/03_quant/#231-building-the-quantification-process","title":"2.3.1 Building the <code>QUANTIFICATION</code> process","text":""},{"location":"part2/03_quant/#defining-the-process-directives","title":"Defining the process directives","text":"<p>Here is the empty <code>process</code> template with the <code>container</code> and <code>publishDir</code> directives we'll be using to get you started. Add this to your <code>main.nf</code> after where you defined the <code>FASTQC</code> process.  </p> main.nf<pre><code>process QUANTIFICATION {\n  container \"quay.io/biocontainers/salmon:1.10.1--h7e5ed60_0\"\n  publishDir \"results\", mode: 'copy'\n\n  input:\n    &lt; process inputs &gt;\n\n  output:\n    &lt; process outputs &gt;\n\n  script:\n  \"\"\"\n    &lt; script to be executed &gt;\n  \"\"\"\n}\n</code></pre> <p>It contains: </p> <ul> <li>Prefilled process directives <code>container</code> and <code>publishDir</code>.</li> <li>The empty <code>input:</code> block for us to define the input data for the process. </li> <li>The empty <code>output:</code> block for us to define the output data for the process.</li> <li>The empty <code>script:</code> block for us to define the script for the process.</li> </ul>"},{"location":"part2/03_quant/#defining-the-process-script","title":"Defining the process <code>script</code>","text":"<p>Update the <code>script</code> definition with the Salmon command from the bash script. Again, we want to remove the definition of the Bash variables and just keep the <code>salmon</code> command:</p> main.nf<pre><code>process QUANTIFICATION {\n  container \"quay.io/biocontainers/salmon:1.10.1--h7e5ed60_0\"\n  publishDir \"results\", mode: 'copy'\n\n  input:\n    &lt; process inputs &gt;\n\n  output:\n    &lt; process outputs &gt;\n\n  script:\n  \"\"\"\n  salmon quant --libType=U -i $salmon_index -1 $reads_1 -2 $reads_2 -o $sample_id\n  \"\"\"\n}\n</code></pre> <p>The <code>--libType=U</code> is a required argument and can be left as is for the script definition. It can stay the same as in the bash script. The following need to be adjusted for the <code>QUANTIFICATION</code> process: </p> <ul> <li><code>-i $salmon_index</code> uses the Nextflow variable <code>salmon_index</code> to refer to the output directory from the <code>INDEX</code> process.</li> <li><code>-1 $reads_1</code> and <code>-2 $reads_2</code> use two more variables to refer to the FASTQ files from the previously defined <code>reads_ch</code> channel. </li> <li><code>-o $sample_id</code> outputs files into a directory named after the <code>$sample_id</code>.</li> </ul>"},{"location":"part2/03_quant/#defining-the-process-output","title":"Defining the process <code>output</code>","text":"<p>Salmon creates an output directory with whatever name is passed to the <code>-o</code> flag. In our case, we supplied <code>-o $sample_id</code> to Salmon, so our <code>output</code> will be a directory named after whatever is in the variable <code>sample_id</code>. For our test sample, the directory will be called <code>gut/</code>. Replace <code>&lt; process outputs &gt;</code> with the following:  </p> main.nf<pre><code>process QUANTIFICATION {\n  container \"quay.io/biocontainers/salmon:1.10.1--h7e5ed60_0\"\n  publishDir \"results\", mode: 'copy'\n\n  input:\n    &lt; process inputs &gt;\n\n  output:\n  path \"$sample_id\"\n\n  script:\n  \"\"\"\n  salmon quant --libType=U -i $salmon_index -1 $reads_1 -2 $reads_2 -o $sample_id\n  \"\"\"\n}\n</code></pre> <p>Now our process will expect to find an output path with the same name as the <code>sample_id</code> variable.</p>"},{"location":"part2/03_quant/#defining-the-process-input","title":"Defining the process <code>input</code>","text":"<p>In this step we will define the process inputs. Based on the bash script, we have four inputs:  </p> <ul> <li><code>$salmon_index</code></li> <li><code>$sample_id</code></li> <li><code>$reads_1</code></li> <li><code>$reads_2</code></li> </ul> <p>These should look familiar! </p> <p>The <code>$salmon_index</code> was output by the <code>INDEX</code> process, and <code>$sample_id</code>, <code>$reads_1</code>, <code>$reads_2</code> are output by our <code>reads_in</code>. We will see how to chain these when we work on the <code>workflow</code> scope below.</p> <p>For this process, we will be defining two inputs. Nextflow allows processes to be defined with any number of inputs.</p> <p>First, add the input definition for <code>$salmon_index</code>. Recall that we use the <code>path</code> qualifier as it is a directory:  </p> main.nf<pre><code>process QUANTIFICATION {\n  container \"quay.io/biocontainers/salmon:1.10.1--h7e5ed60_0\"\n  publishDir \"results\", mode: 'copy'\n\n  input:\n  path salmon_index\n\n  output:\n  path \"$sample_id\"\n\n  script:\n  \"\"\"\n  salmon quant --libType=U -i $salmon_index -1 $reads_1 -2 $reads_2 -o $sample_id\n  \"\"\"\n}\n</code></pre> <p>Secondly, add the tuple input:  </p> main.nf<pre><code>process QUANTIFICATION {\n  container \"quay.io/biocontainers/salmon:1.10.1--h7e5ed60_0\"\n  publishDir \"results\", mode: 'copy'\n\n  input:\n  path salmon_index\n  tuple val(sample_id), path(reads_1), path(reads_2)\n\n  output:\n  path \"$sample_id\"\n\n  script:\n  \"\"\"\n  salmon quant --libType=U -i $salmon_index -1 $reads_1 -2 $reads_2 -o $sample_id\n  \"\"\"\n}\n</code></pre> <p>You have just defined a process with multiple inputs!  </p> <p>How's it going?</p> <p>Once you have defined the <code>process</code> block, select the  \"Yes\" react on Zoom.</p>"},{"location":"part2/03_quant/#calling-the-process-in-the-workflow-scope","title":"Calling the process in the <code>workflow</code> scope","text":"<p>Recall that the inputs for the <code>QUANTIFICATION</code> process are emitted by the <code>reads_in</code> channel and the output of the <code>INDEX</code> process. The <code>reads_in</code> channel  is ready to be called by the <code>QUANTIFICATION</code> process. Similarly, we need to prepare a channel for the index files output by the <code>INDEX</code> process.</p> <p>Add the following channel to your <code>main.nf</code> file, after the <code>FASTQC</code> process:</p> main.nf<pre><code>// Define the workflow\nworkflow {\n\n    // Run the index step with the transcriptome parameter\n    INDEX(params.transcriptome_file)\n\n    // Define the fastqc input channel\n    reads_in = Channel.fromPath(params.reads)\n        .splitCsv(header: true)\n        .map { row -&gt; [row.sample, file(row.fastq_1), file(row.fastq_2)] }\n\n    // Run the fastqc step with the reads_in channel\n    FASTQC(reads_in)\n\n    // Define the quantification channel for the index files\n    transcriptome_index_in = INDEX.out[0]\n\n}\n</code></pre> <p>The syntax <code>INDEX.out[0]</code> specifies that we want to access the first output of the <code>INDEX</code> process. Numbering starts at zero, so <code>[0]</code> refers to the first output, <code>[1]</code> to the second, and so forth.</p> Advanced content: Different ways of accessing process outputs <p>Nextflow allows us to access the output of a process using the <code>.out</code> attribute. If a process has a single output, you can simply use <code>&lt;PROCESS_NAME&gt;.out</code>. If a process has 2 or more output channels, you need to use an integer index to access the corresponding outputs. An index is simply a number - starting at <code>0</code> - that indicates the place of an item within a list or array. We use square brackets to access values using an index - e.g. <code>[0]</code>, <code>[1]</code>, etc. Process outputs are ordered within the <code>.out</code> attribute based on the order in which they were defined. For example: <code>.out[0]</code> will access the first defined output, <code>.out[1]</code> will access the second output, and so forth.</p> <p>For the sake of consistency, we have used the indexed method here (i.e. <code>INDEX.out[0]</code>), although we could have omitted the index and simply used <code>INDEX.out</code> since our <code>INDEX</code> process only has the single output.</p> <p>Alternatively, the process output definition allows the use of the <code>emit</code> statement to define a named identifier that can be used to reference the channel in the external scope:</p> process<pre><code>...\noutput:\npath \"salmon_index\", emit: index\n...\n</code></pre> workflow<pre><code>...\ntranscriptome_index_in = INDEX.out.index\n...\n</code></pre> <p>Call the <code>QUANTIFICATION</code> process in the workflow scope and add the inputs by adding the following line to your <code>main.nf</code> file after your <code>transcriptome_index_in</code> channel definition:  </p> main.nf<pre><code>// Define the workflow\nworkflow {\n\n    // Run the index step with the transcriptome parameter\n    INDEX(params.transcriptome_file)\n\n    // Define the fastqc input channel\n    reads_in = Channel.fromPath(params.reads)\n        .splitCsv(header: true)\n        .map { row -&gt; [row.sample, file(row.fastq_1), file(row.fastq_2)] }\n\n    // Run the fastqc step with the reads_in channel\n    FASTQC(reads_in)\n\n    // Define the quantification channel for the index files\n    transcriptome_index_in = INDEX.out[0]\n\n    // Run the quantification step with the index and reads_in channels\n    QUANTIFICATION(transcriptome_index_in, reads_in)\n\n}\n</code></pre> <p>By doing this, we have passed two arguments to the <code>QUANTIFICATION</code> process as there are two inputs in the <code>process</code> definition. </p> <p>Matching process inputs</p> <p>Recall that the number of inputs in the process input block and the workflow must match!</p> <p>If you have multiple inputs they need to be listed across multiple lines in the process input block and listed inside the parentheses and separated by commas in the workflow block.</p> <p>Run the workflow:  </p> <pre><code>nextflow run main.nf -resume\n</code></pre> <p>Your output should look like:  </p> Output<pre><code>Launching `main.nf` [shrivelled_cuvier] DSL2 - revision: 4781bf6c41\n\nexecutor &gt;  local (1)\n[de/fef8c4] INDEX              | 1 of 1, cached: 1 \u2714\n[bb/32a3aa] FASTQC (1)         | 1 of 1, cached: 1 \u2714\n[a9/000f36] QUANTIFICATION (1) | 1 of 1 \u2714\n</code></pre> <p>A new <code>QUANTIFICATION</code> task has been successfully run and have a <code>results/gut</code> folder, with an assortment of files and directories. </p> <p>Summary</p> <p>In this lesson you have learned:  </p> <ol> <li>How to define a process with multiple input channels</li> <li>How to access a process output with <code>.out</code></li> <li>How to create a channel from a process output</li> <li>How to chain Nextflow processes with channels  </li> </ol>"},{"location":"part2/04_multiqc/","title":"2.4 Combining channels and multiple process outputs","text":"<p>Learning objectives</p> <ol> <li>Construct a channel that merges the contents of two existing channels.  </li> <li>Implement a Nextflow process that produces multiple output files and defines them approporiately. </li> </ol> <p>In this step we will transform the <code>03_multiqc.sh</code> into a process called <code>MULTIQC</code>.  This step focuses on the final step of our RNAseq data processing workflow: generating a report that summarises the quality control and quantification steps. </p> <p>To do this, we will run MultiQC, which is a popular tool  for summarising the outputs of many different bioinformatics tools. It aggregates results from all our analyses and renders it into a nice report. </p> <p>From the MultiQC docs</p> <p>MultiQC doesn\u2019t do any analysis for you - it just finds results from other tools that you have already run and generates nice reports. See here for a list of supported tools.</p> <p> </p> <p>Open the bash script <code>03_multiqc.sh</code>.  </p> 03_multiqc.sh<pre><code>multiqc --outdir results/ results/\n</code></pre> <p>This script is a lot simpler than previous scripts we've worked with. It searches for the output files generated by the <code>FASTQC</code> and <code>QUANTIFICATION</code> processes saved to the <code>results/</code> directory. As specified by <code>--outdir results/</code>, it will then output two MultiQC files to the same <code>results/</code> directory:  </p> <ol> <li>A directory called <code>multiqc_data/</code> </li> <li>A report file called <code>multiqc_report.html</code> </li> </ol>"},{"location":"part2/04_multiqc/#241-building-the-multiqc-process","title":"2.4.1 Building the <code>MULTIQC</code> process","text":""},{"location":"part2/04_multiqc/#process-directives-script-and-input","title":"Process directives, <code>script</code>, and <code>input</code>","text":"<p>Here is the <code>process</code> template with the <code>container</code> and <code>publishDir</code> directives provided. Add this to your <code>main.nf</code> after the <code>QUANTIFICATION</code> process:  </p> main.nf<pre><code>process MULTIQC {\n  container \"quay.io/biocontainers/multiqc:1.19--pyhdfd78af_0\"\n  publishDir \"results\", mode: 'copy'\n\n  input:\n  path \"*\"\n\n  output:\n    &lt; process outputs &gt;\n\n  script:\n  \"\"\"\n  multiqc .\n  \"\"\"\n}\n</code></pre> <p>The <code>script</code> and <code>input</code> follow the MultiQC Nextflow integration recommendations.  The key thing to note here is that MultiQC needs to be run once for all upstream outputs. </p> <p>From the information above we know that for the original bash script the inputs for <code>multiqc</code> are the files and directories within the <code>results/</code> directory. Specifically, the inputs are the outputs from the <code>fastqc</code> and <code>salmon quant</code> commands. To replicate this for our new process, we will need to bring the outputs of the <code>FASTQC</code> (<code>fastqc_gut_logs/</code>) and <code>QUANTIFICATION</code> (<code>gut/</code>) processes into a single channel as input to <code>MULTIQC</code>.  </p> <p>Why you should NOT use the <code>publishDir</code> folder as a process input</p> <p>It might make sense to have the <code>results/</code> folder (set by <code>publishDir</code>) as the input to the process here, but it may not exist until the workflow finishes.</p> <p>Using the <code>publishDir</code> as a process input can cause downstream processes to run prematurely, even if the directory is empty or incomplete. In this case,  MultiQC might miss some inputs.</p> <p>Always use channels to pass data between processes. Channels enable Nextflow to track outputs and ensure that downstream processes only run when all required data is ready, maintaining proper worfklow control.</p> <p>We will expand more on how to bring these outputs together in the next section.</p> <p>For now, note how our input definition is a bit different to previous processes. Instead of defining a variable name to represent our input, we are defining our input as <code>path \"*\"</code>.</p> <p>This syntax tells Nextflow to simply expect any number of files or directories as input. With this syntax, we don't get a variable to use within our script, but that's OK, because MultiQC just needs to be given a directory to search (in our case, the current directory, represented by <code>.</code>) and it will automatically find all the relevant data.</p>"},{"location":"part2/04_multiqc/#defining-the-process-output","title":"Defining the process <code>output</code>","text":"<p>The MultiQC output consists of the following:</p> <ul> <li>An HTML report file called <code>multiqc_report.html</code></li> <li>A directory called <code>multiqc_data/</code> containing the data used to generate the report.</li> </ul> <p>Add the following <code>output</code> definition to the <code>MULTIQC</code> process:  </p> main.nf<pre><code>process MULTIQC {\n\n  container \"quay.io/biocontainers/multiqc:1.19--pyhdfd78af_0\"\n  publishDir \"results\", mode: 'copy'\n\n  input:\n  path \"*\"  \n\n  output:\n  path \"multiqc_report.html\"\n  path \"multiqc_data\"\n\n  script:\n  \"\"\"\n  multiqc .\n  \"\"\"\n}\n</code></pre> <p>You have now defined a process with multiple outputs!</p>"},{"location":"part2/04_multiqc/#242-combining-channels-with-operators","title":"2.4.2 Combining channels with operators","text":"<p>Tip</p> <p>When running MultiQC, it needs to be run once on all the upstream input files. This is so a single report is generated with all the results.</p> <p>The input files for our <code>MULTIQC</code> process will be the outputs from the <code>FASTQC</code> and <code>QUANTIFICATION</code> processes. Both FastQC and Salmon are supported by MultiQC and the required files are detected automatically by the program. However, in order to use MultiQC within our Nextflow pipeline, we will first need to do some pre-processing to get all of our data in one place for MultiQC to search through.</p> <p>The goal of this step is to bring the outputs from <code>MULTIQC</code> and <code>QUANTIFICATION</code> processes into a single input channel for the <code>MULTIQC</code> process. This ensures that MultiQC is run once.  </p> <p>The next few additions will involve chaining together Nextflow operators to correctly format inputs for the <code>MULTIQC</code> process.  </p> <p>Poll</p> <p>What Nextflow input type (qualifier) ensures that inputs are grouped and processed together?</p> <p>Add the following to the workflow block in your <code>main.nf</code> file, under the call to the <code>QUANTIFICATION</code> process.  </p> main.nf<pre><code>// Define the workflow\nworkflow {\n\n    // Run the index step with the transcriptome parameter\n    INDEX(params.transcriptome_file)\n\n    // Define the fastqc input channel\n    reads_in = Channel.fromPath(params.reads)\n        .splitCsv(header: true)\n        .map { row -&gt; [row.sample, file(row.fastq_1), file(row.fastq_2)] }\n\n    // Run the fastqc step with the reads_in channel\n    FASTQC(reads_in)\n\n    // Define the quantification channel for the index files\n    transcriptome_index_in = INDEX.out[0]\n\n    // Run the quantification step with the index and reads_in channels\n    QUANTIFICATION(transcriptome_index_in, reads_in)\n\n    // Define the multiqc input channel\n    FASTQC.out[0]\n        .mix(QUANTIFICATION.out[0])\n        .view()\n\n}\n</code></pre> <p>This channel does the following:</p> <ul> <li>Takes the output of <code>FASTQC</code>, using element <code>[0]</code> to refer to the first element of the output. </li> <li>Uses <code>mix(QUANTIFICATION.out[0])</code> to combine <code>FASTQC.out[0]</code> output with the first element of the <code>QUANTIFICATION</code> output.</li> <li>Uses <code>view()</code> allows us to see the values emitted into the channel.</li> </ul> <p>For more information, see the documentation on <code>mix</code>.</p> <p>Run the workflow to see what it produces:  </p> <pre><code>nextflow run main.nf -resume  \n</code></pre> <p>The output should look something like:  </p> Output<pre><code>Launching `main.nf` [stupefied_minsky] DSL2 - revision: 82245ce02b\n\n[de/fef8c4] INDEX              | 1 of 1, cached: 1 \u2714\n[bb/32a3aa] FASTQC (1)         | 1 of 1, cached: 1 \u2714\n[a9/000f36] QUANTIFICATION (1) | 1 of 1, cached: 1 \u2714\n/home/user1/part2/work/bb/32a3aaa5e5fd68265f0f34df1c87a5/fastqc_gut_logs\n/home/user1/part2/work/a9/000f3673536d98c8227b393a641871/gut\n</code></pre> <p>The outputs have been emitted one after the other. This means that they are currently two separate elements within the channel, which will cause them to be processed separately. We really need them to be processed together (generated in the same MultiQC report). What we need is for them to both be contained within a single tuple, so we need to add one more step.  </p> <p>Note</p> <p>Note that the outputs point to the files in the work directories, rather than the <code>publishDir</code>. This is one of the ways that Nextflow ensures all input files are ready and ensures proper workflow control.</p> <p>The <code>collect</code> operator is perfect for this. It takes all of the elements in a channel and outputs a single tuple containg everything - just what we need! Add a <code>collect</code> operator to the chain of operators to ensure that all samples are processed together in the same process and view the output:  </p> main.nf<pre><code>// Define the workflow\nworkflow {\n\n    // Run the index step with the transcriptome parameter\n    INDEX(params.transcriptome_file)\n\n    // Define the fastqc input channel\n    reads_in = Channel.fromPath(params.reads)\n        .splitCsv(header: true)\n        .map { row -&gt; [row.sample, file(row.fastq_1), file(row.fastq_2)] }\n\n    // Run the fastqc step with the reads_in channel\n    FASTQC(reads_in)\n\n    // Define the quantification channel for the index files\n    transcriptome_index_in = INDEX.out[0]\n\n    // Run the quantification step with the index and reads_in channels\n    QUANTIFICATION(transcriptome_index_in, reads_in)\n\n    // Define the multiqc input channel\n    FASTQC.out[0]\n        .mix(QUANTIFICATION.out[0])\n        .collect()\n        .view()\n\n}\n</code></pre> <p>Run the workflow:  </p> <pre><code>nextflow run main.nf -resume  \n</code></pre> <p>The channel now outputs a single tuple with the two directories:  </p> Output<pre><code>Launching `main.nf` [small_austin] DSL2 - revision: 6ab927f137\n\n[de/fef8c4] INDEX              | 1 of 1, cached: 1 \u2714\n[bb/32a3aa] FASTQC (1)         | 1 of 1, cached: 1 \u2714\n[a9/000f36] QUANTIFICATION (1) | 1 of 1, cached: 1 \u2714\n[/home/user1/part2/work/bb/32a3aaa5e5fd68265f0f34df1c87a5/fastqc_gut_logs, /home/user1/part2/work/a9/000f3673536d98c8227b393a641871/gut]\n</code></pre> <p>Now that we have a channel that emits data in the correct format, add the finishing touches to the workflow scope.</p> <p>Exercise: Assign the input channel</p> <ol> <li>Assign the chain of operations to a channel called <code>multiqc_in</code></li> <li>Remove the <code>.view()</code> operator  </li> </ol> Solution main.nf<pre><code>    // Define the quantification channel for the index files\n    transcriptome_index_in = INDEX.out[0]\n\n    // Run the quantification step with the index and reads_in channels\n    QUANTIFICATION(transcriptome_index_in, reads_in)\n\n    // Define the multiqc input channel\n    multiqc_in = FASTQC.out[0]\n        .mix(QUANTIFICATION.out[0])\n        .collect()\n\n}\n</code></pre> <p>Exercise: Call the <code>MULTIQC</code> process</p> <ol> <li>Add a call to the <code>MULTIQC</code> process in the workflow scope</li> <li>Pass the <code>multiqc_in</code> channel as input.</li> </ol> Solution main.nf<pre><code>    // Define the quantification channel for the index files\n    transcriptome_index_in = INDEX.out[0]\n\n    // Run the quantification step with the index and reads_in channels\n    QUANTIFICATION(transcriptome_index_in, reads_in)\n\n    // Define the multiqc input channel\n    multiqc_in = FASTQC.out[0]\n        .mix(QUANTIFICATION.out[0])\n        .collect()\n\n    // Run the multiqc step with the multiqc_in channel\n    MULTIQC(multiqc_in)\n\n}\n</code></pre> <p>Run the workflow:  </p> <pre><code>nextflow run main.nf -resume  \n</code></pre> <p>Your output should look something like:  </p> Output<pre><code>Launching `main.nf` [hopeful_swanson] DSL2 - revision: a4304bbe73\n\n[aa/3b8821] INDEX          [100%] 1 of 1, cached: 1 \u2714\n[c2/baa069] QUANTIFICATION [100%] 1 of 1, cached: 1 \u2714\n[ad/e49b20] FASTQC         [100%] 1 of 1, cached: 1 \u2714\n[a3/1f885c] MULTIQC        [100%] 1 of 1 \u2714\n</code></pre>"},{"location":"part2/04_multiqc/#243-inspecting-the-multiqc-report","title":"2.4.3 Inspecting the MultiQC report","text":"<p>Let's inspect the generated MultiQC report. You will need to download the file to your local machine and open it in a web browser.  </p> <p>Exercise</p> <ol> <li>In the VSCode Explorer sidebar, locate the report <code>results/multiqc_report.html</code> </li> <li>Right click on the file, and select \"Download\"</li> <li>Open the file in a web browser</li> </ol> <p>Poll</p> <p>Under the \"General Statistics\" section, how many rows have been included in the table?</p> <p>Tip</p> <p>If you have to view many <code>.html</code> files on a remote server, we recommend using the  Live Server VSCode extension. </p> <p>The extension allows you to view <code>.html</code> files within a VSCode tab instead of manually downloading files locally.</p> <p>You now have a working pipeline for a single paired-end sample!</p> <p>Summary</p> <p>In this lesson you have learned:</p> <ol> <li>How to implement a process following integration recommendations</li> <li>How to define an output with multiple outputs</li> <li>How to use the <code>mix</code> and <code>collect</code> operators to combine outputs into a single tuple</li> <li>How to access and view <code>.html</code> files from a remote server</li> </ol>"},{"location":"part2/05_scale/","title":"2.5 Productionising our workflow","text":"<p>Learning objectives</p> <ol> <li>Configure Nextflow workflows to run on multiple samples</li> <li>Enable and interpret Nextflow's inbuilt reports </li> <li>Use the <code>tag</code> directive to label tasks for better tracking and profiling</li> <li>Configure a Nextflow process to request and use multiple CPUs for a process </li> </ol> <p>Now that we have a working pipeline on a single-sample, we will update it  to take multiple samples and introduce Nextflow concepts that not only help with understanding and profiling the pipeline but also set the stage for productionising it. </p> <p>We will focus on making the workflow scalable, robust, and efficient for real-world data processing. Key productionisation practices include: </p> <ul> <li>Automating tasks</li> <li>Handling errors gracefully</li> <li>Optimising resource usage</li> <li>Ensuring reproducibility. </li> </ul> <p>These steps ensure that the pipeline can be reliably used in more complex scenarios, like when processing multiple samples in parallel. </p>"},{"location":"part2/05_scale/#251-labeling-tasks-with-the-tag-directive","title":"2.5.1 Labeling tasks with the <code>tag</code> directive","text":"<p>The tag process directive allows you to add a custom label, or tag, to each task that gets executed. It is useful for identifying what is being run when the workflow is being executed in a bit more detail. It is especially helpful showing you  what is being run when we run multiple samples, and for profiling later.</p> <p>Add the following <code>tag</code> directives to your existing <code>FASTQC</code> and <code>QUANTIFICATION</code> processes. </p> <p>For <code>FASTQC</code>:</p> main.nf<pre><code>process FASTQC {\n    tag \"fastqc on ${sample_id}\"\n    container \"quay.io/biocontainers/fastqc:0.12.1--hdfd78af_0\"\n    publishDir \"results\", mode: 'copy'\n</code></pre> <p>And for <code>QUANTIFICATION</code>:  </p> main.nf<pre><code>process QUANTIFICATION {\n    tag \"salmon on ${sample_id}\"\n    container \"quay.io/biocontainers/salmon:1.10.1--h7e5ed60_0\"\n    publishDir \"results\", mode: 'copy'\n</code></pre> <p>The tags we just added indicates what program is being run (<code>fastqc</code> or  <code>salmon</code>), and on which sample (<code>${sample_id}</code>) it is being run on. </p> <p>Run the pipeline with the updated tags:  </p> <pre><code>nextflow run main.nf -resume\n</code></pre> <p>The output should look similar to:  </p> Output<pre><code>Launching `main.nf` [distraught_bell] DSL2 - revision: dcb06191e7\n\nexecutor &gt;  local (5)\n[aa/3b8821] INDEX                           | 1 of 1, cached: 1 \u2714\n[c2/baa069] FASTQC (fastqc on gut)          | 1 of 1, cached: 1 \u2714\n[ad/e49b20] QUANTIFICATION (salmon on gut)  | 1 of 1, cached: 1 \u2714\n[a3/1f885c] MULTIQC                         | 1 of 1, cached: 1 \u2714\n</code></pre> <p>No new tasks were run, but <code>FASTQC</code> and <code>QUANTIFICATION</code> processes now have labels appended in the execution output.  </p>"},{"location":"part2/05_scale/#252-using-a-samplesheet-with-multiple-samples","title":"2.5.2 Using a samplesheet with multiple samples","text":"<p>Recall that the samplesheet is used to control which files/data are analysed by the workflow. Inspect <code>data/samplesheet_full.csv</code>.  </p> samplesheet_full.csv<pre><code>sample,fastq_1,fastq_2\ngut,data/ggal/gut_1.fq,data/ggal/gut_2.fq\nliver,data/ggal/liver_1.fq,data/ggal/liver_2.fq\nlung,data/ggal/lung_1.fq,data/ggal/lung_2.fq\n</code></pre> <p>Compared to the samplesheet we have been using (<code>data/samplesheet.csv</code>), this one contains two additional lines for the <code>liver</code> and <code>lung</code> paired reads.</p> <p>Next we will run the workflow with all three samples by overwriting the default  input for <code>reads</code> with <code>data/samplesheet_full.csv</code> using the double hyphen  approach <code>--reads</code> in the run command.</p> <p>Run the workflow:  </p> <pre><code>nextflow run main.nf -resume --reads data/samplesheet_full.csv\n</code></pre> <p>Your output should look similar to:  </p> Output<pre><code>Launching `main.nf` [distraught_bell] DSL2 - revision: dcb06191e7\n\nexecutor &gt;  local (5)\n[de/fef8c4] INDEX                           | 1 of 1, cached: 1 \u2714\n[4e/b4c797] FASTQC (fastqc on liver)        | 3 of 3, cached: 1 \u2714\n[36/93c8b4] QUANTIFICATION (salmon on lung) | 3 of 3, cached: 1 \u2714\n[e7/5d91ea] MULTIQC                         | 1 of 1 \u2714\n</code></pre> <p>There are two new tasks run for <code>FASTQC</code> and <code>QUANTIFICATION</code>. Our newly added tags indicate which samples they were run on - either <code>lung</code> or <code>liver</code> reads!</p> Advanced Exercise: Inspecting the <code>reads_in</code> channel with multiple samples <ol> <li>Update the workflow scope to inspect the output of the <code>reads_in</code> channel (i.e. with <code>.view()</code>)</li> <li>Run the workflow with <code>samplesheet_full.csv</code></li> </ol> <p>What has changed with what the <code>reads_in</code> channel is emitting?</p> Solution <p>Viewing <code>reads_in</code>: </p> main.nf<pre><code>    // Define the fastqc input channel\n    reads_in = Channel.fromPath(params.reads)\n        .splitCsv(header: true)\n        .map { row -&gt; [row.sample, file(row.fastq_1), file(row.fastq_2)] }\n\n    reads_in.view()\n\n    // Run the fastqc step with the reads_in channel\n    FASTQC(reads_in)\n</code></pre> <p>Run the workflow:  </p> <pre><code>nextflow run main.nf -resume --reads data/samplesheet_full.csv\n</code></pre> <p>Your output should look something like:  </p> Output<pre><code>executor &gt;  local (5)\n[de/fef8c4] INDEX                           | 1 of 1, cached: 1 \u2714\n[4e/b4c797] FASTQC (fastqc on liver)        | 3 of 3, cached: 3 \u2714\n[36/93c8b4] QUANTIFICATION (salmon on lung) | 3 of 3, cached: 3 \u2714\n[e7/5d91ea] MULTIQC                         | 1 of 1 \u2714\n[gut, .../data/ggal/gut_1.fq, .../data/ggal/gut_2.fq]\n[liver, .../data/ggal/liver_1.fq, .../data/ggal/liver_2.fq]\n[lung, .../data/ggal/lung_1.fq, .../data/ggal/lung_2.fq]\n</code></pre> <p>There are now a total of three tuples emitted separately for each sample.  When passed into <code>FASTQC</code> and <code>QUANTIFICATION</code>, each tuple is processed separately in independent tasks.</p> <p>Remove <code>reads_in.view()</code> before proceeding.</p>"},{"location":"part2/05_scale/#253-an-introduction-to-configuration","title":"2.5.3 An introduction to configuration","text":"<p>In this section, we will explore how Nextflow workflows can be configured to utilise the computational resources available. Whilst there are many ways to configure Nextflow workflows (especially on HPC clusters), we will focus on increasing the number of CPUs used to speed up tasks.  </p> <p>Some bioinformatics tool, like FastQC, support multithreading to speed up analyses. From the <code>fastqc --help</code> command, you'll notice the following option:</p> Output<pre><code>-t --threads    Specifies the number of files which can be processed    \n                simultaneously.\n</code></pre> <p>This means we can configure the number of threads (or CPUs) that FastQC uses to process multiple files in parallel to speed up the analysis. In Nextflow, we control this through the  <code>cpus</code> directive.</p> <p>Recall that our <code>FASTQC</code> takes as input the <code>reads_in</code> channel which emits two <code>.fastq</code> files. We will configure the process to use 2 CPUs so each file gets run on 1 CPU each (the maximum CPUs fastqc will use per file), simulataneously.</p> <p>In your <code>main.nf</code> script, update the <code>script</code> definition in the <code>FASTQC</code> process to add the multithreading option:  </p> main.nf<pre><code>    script:\n    \"\"\"\n    mkdir fastqc_${sample_id}_logs\n    fastqc --outdir \"fastqc_${sample_id}_logs\" -f fastq $reads_1 $reads_2 -t $task.cpus\n    \"\"\"\n</code></pre> <ul> <li>The <code>task.cpus</code> variable is automatically populated with the number of  CPUs allocated to the task based on the Nextflow configuration. By default this is 1.  </li> </ul> <p>Next, we need to update our <code>nextflow.config</code> file to configure the number of CPUs to be used. To allow each FastQC process to use 2 CPUs, update the config file as follows:  </p> nextflow.config<pre><code>process.cpus = 2\n\nsingularity {\n    enabled = true\n    cacheDir = \"$HOME/singularity_image\"\n}\n</code></pre> <p>The <code>-t $task.cpus</code> argument will populate as <code>-t 2</code> when we run the workflow next. Before we do, we will explore Nextflow's built-in reporting system to assess resource usage.</p>"},{"location":"part2/05_scale/#254-inspecting-workflow-performance","title":"2.5.4 Inspecting workflow performance","text":"<p>When running workflows, it is helpful to understand how each part of your workflow is using resources like CPUs, memory, and the time taken to complete. Nextflow can generate text-based and visual reports that give you clear picture of how your workflow ran and identify areas for improvement.  </p> <p>We will explore some of Nextflow's built-it in tools that can show these important details of how tasks ran.</p> <p>To enable these reports, add the following to your <code>nextflow.config</code> file:</p> nextflow.config<pre><code>process.cpus = 2\n\nsingularity {\n    enabled = true\n    cacheDir = \"$HOME/singularity_image\"\n}\n\n// enable reporting\ndag.enabled = true\nreport.enabled = true\ntimeline.enabled = true\ntrace.enabled = true\n</code></pre> <p>Run the workflow. To assess the resource usage all processes need to be run again so <code>-resume</code> should not be used. (If we resume now, it will still appear as a cached run, with limited information).  </p> <pre><code>nextflow run main.nf --reads \"data/samplesheet_full.csv\"\n</code></pre> <p>Inspect your project directory. You should have 3 <code>.html</code> files and a <code>.txt</code> file with matching timestamps. A summary of the different reports are included in the table below. For a detailed description of each report see the Nextflow documentation on reports.</p> Report type Description <code>dag</code> A high-level graph that shows how processes and channels are connected to each other. <code>report</code> A visual summary of the time and resources used grouped by process. <code>timeline</code> A Gannt chart that shows when each task started and ended. <code>trace</code> A detailed text log with the time and resources used by each task. <p>Complete the following steps in the exercise to view the report file <code>report-*.html</code> in your local browser.  </p> <p>Exercise</p> <ol> <li>In the VSCode file explorer sidebar, locate the report file (e.g. <code>report-*.html</code>)</li> <li>Right click on the file and select \"Download\" to save it to your local computer.</li> <li>Open the <code>report-*.html</code> in a browser.</li> <li>Navigate to \"Resource Usage\" -&gt; \"CPU\".</li> <li>Hover over the <code>FASTQC</code> bar chart and note the <code>mean</code> CPU usage.</li> </ol> <p>Poll</p> <p>What was the <code>mean</code> CPU usage for your <code>FASTQC</code> process?</p> Solution <p>In this report, a mean of 2.53 CPUs were utilised by the <code>FASTQC</code> process across the 3 samples. This value will slightly differ across runs.</p> <p></p> <p>You have successfully run, configured, and profiled a multi-sample workflow!</p> <p>Summary</p> <p>In this lesson you have learned:</p> <ol> <li>How to add custom labels with process tags</li> <li>How to use <code>task.cpus</code> to enable multithreading within processes</li> <li>How to configure process resources with <code>nextflow.config</code></li> <li>How to enable and view Nextflow workflow reports</li> </ol>"}]}